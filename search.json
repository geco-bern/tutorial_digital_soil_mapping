[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AGDS 2: Digital Soil Mapping",
    "section": "",
    "text": "Preface\nThis online book covers the tutorial on digital soil mapping from the course Applied Geodata Science 2 at the University of Bern. Code and data was provided by Madlene Nussbaum and the conversion into an online tutorial was done by Pascal Schneider.\n\n\nSystem Setup\nIf you want to replicate the code of this tutorial, we suggest to either load the latest renv.lock file from this tutorial’s repository and load it using the {renv} package. Alternatively, the output below could be used to traceback and install the exact package versions that were used in this tutorial.\n\n\nCode\nsessionInfo()\n\n\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Zurich\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.1    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.1       htmltools_0.5.5   rstudioapi_0.15.0 rmarkdown_2.23   \n [9] knitr_1.43        jsonlite_1.8.7    xfun_0.39         digest_0.6.33    \n[13] rlang_1.1.1       evaluate_0.21"
  },
  {
    "objectID": "01-introduction.html#a-primer-on-spatial-data-science",
    "href": "01-introduction.html#a-primer-on-spatial-data-science",
    "title": "1  Introduction",
    "section": "1.1 A primer on spatial data science",
    "text": "1.1 A primer on spatial data science\nSpatial data science combines geography, statistics, computer science, and data science to analyze and interpret spatially referenced data. It focuses on uncovering patterns and relationships in geospatial data to gain insights into spatial phenomena. By integrating locational information, spatial data science provides a deeper understanding of complex spatial patterns and processes. There are three key aspects of spatial data science that allow such a deeper understanding:\n\nSpatial Data Visualization: Visualizing spatial data through maps and interactive visualizations helps communicate complex spatial information effectively. A brief introduction to working with geospatial data in R is given here.\nSpatial Data Analysis: Techniques such as spatial clustering, spatial autocorrelation analysis, and spatial regression reveal spatial patterns, trends, and dependencies.\nGeospatial Machine Learning: Applying machine learning algorithms to spatial data enables the creation of predictive models for spatially explicit predictions.\n\nCombined, working on each of these aspects allows for a variety of real-world applications. For example, in urban planning, ecology, transportation, public health, and social sciences, we can apply similar methods to solve a problem. The knowledge inferred from maps, statistical analysis, and model prediction reveals fundamental processes to understand spatial relationships and to eventually improve decision-making."
  },
  {
    "objectID": "01-introduction.html#a-primer-on-soil-science",
    "href": "01-introduction.html#a-primer-on-soil-science",
    "title": "1  Introduction",
    "section": "1.2 A primer on soil science",
    "text": "1.2 A primer on soil science\nGenerally, any soil is the result of five key pedogenetic factors (Jenny, 1994). Abbreviated, they can be simply memorized with the mnemonic “CLORPT”: soil = f(CLimate, Organisms, topogRaphy, Parent material, Time, … ). The “…” stands for additional factors, which have been recognized to be important factors but do not fall under the original CLORPT scheme. soil can stand for various soil properties like its texture, density, pH, water drainage, cation exchange capacity, organic matter content, etc.\nDue to the intensification of land use through agriculture and urbanization, soils across the world are increasingly under threat. Yet, soils provide crucial ecosystem services to us, such as supporting our food system, draining water during heavy rainfalls, and storing massive amounts of carbon (see Figure 1.1). To quantify such services and assess the risks associated with losing soils, good maps are needed to provide information on where soil service is delivered. Creating such maps through exhaustive sampling campaigns is very costly, time-intensive, and often at coarse temporal and spatial resolution. Also, such sampling campaigns provide only a snapshot of the historical and current state - they obviously cannot tell us anything about the soil’s future. So, there is a strong demand for models that provide us with information on a soil’s future across large spatial scales.\n\n\n\n\n\nFigure 1.1: Soil functions as defined per FAO, Figure taken from Baveye et al. (2020).\n\n\n\n\nLuckily, information on the CLORPT variables is often available at large continuous scales, for example, spatial data products for climate data, digital elevation models, and geological maps. This information is highly useful for creating models that can predict key soil properties and services across the same area for which we have data. So, if we can produce a robust model, we can massively simplify sampling efforts, remove the need for hand-drawn maps, and inform decision-making processes in a cost- and labor-effective manner.\nThe increase in data abundance and computational resources and advances in statistics have put forward such statistical models. The use of Random Forests has gained a lot of traction due to being relatively simple whilst highly flexible (Hengl et al., 2018). Therefore, they are a perfect match for creating digital maps of all sorts of soil properties in a quick and simple manner.\nHowever, note that the variety in pedogenetic factors and soil properties comes with an equal variety of data types with variables being numerical (capped like % of clay content, or un-capped like organic matter content), binary (e.g., presence of water at 0-10 cm soil depth), categorical (more than two without an order), ordinal (more than two with an order), or interval (cutting numerical values into intervals). Moreover, this data can come in different data formats. Due to this abundance of data formats and their peculiarities, it is of great importance to properly understand your data. Only when you know your data well you can pick a suitable statistical model to address your research question."
  },
  {
    "objectID": "01-introduction.html#case-study-digital-soil-mapping-with-random-forests",
    "href": "01-introduction.html#case-study-digital-soil-mapping-with-random-forests",
    "title": "1  Introduction",
    "section": "1.3 Case-Study: Digital Soil Mapping with Random Forests",
    "text": "1.3 Case-Study: Digital Soil Mapping with Random Forests\nIn this tutorial, we are looking at a specific case of spatial upscaling: digital soil mapping using Random Forests. This means that we want to predict soil properties that are difficult and laborious to obtain with a model that predicts such properties smoothly across space by exploiting available spatial data such as climate data. Here’s a short checklist of what a good geo-spatial model should do. It should…\n\n… capture non-linear relations because pedogenesis is a non-linear process.\n… be able to use and predict continuous and categorical variables.\n… handle multiple correlated variables without the risk of over-fitting.\n\n\n\n… build models with good predictive power.\n… result in a sparse model, keeping only relevant predictors.\n… quantify prediction accuracy and uncertainty.\n\nIn the next chapters, we use a dataset on basic soil properties from sampling locations across the canton of Bern and pair it up with climatic variables (temperature, precipitation, radiation), terrain attributes (derivatives from digital elevation models like slope, northness, eastness, topographic water index, etc.), geological maps, and soil maps (Nussbaum et al., 2018). The following chapters will cover the preparation of this data (Chapter 2), fitting a Random Forest model (Chapter 3), and evaluating this model (Chapter 4). The final Chapter 5 holds the exercise description of this tutorial.\nIf you want to learn more about the underlying theory and similar techniques, we highly recommend the presentations by Madlene Nussbaum given at the summer school of the OpenGeoHub Foundation (see part 1 and part 2) and her papers: Nussbaum et al. (2017), Nussbaum et al. (2018)).\n\n\n\n\nHengl, T., Nussbaum, M., Wright, M. N., Heuvelink, G. B. M., & Gräler, B. (2018). Random forest as a generic framework for predictive modeling of spatial and spatio-temporal variables. PeerJ, 6, e5518. https://doi.org/10.7717/peerj.5518\n\n\nJenny, H. (1994). Factors of soil formation: A system of quantitative pedology. Dover.\n\n\nNussbaum, M., Spiess, K., Baltensweiler, A., Grob, U., Keller, A., Greiner, L., Schaepman, M. E., & Papritz, A. (2018). Evaluation of digital soil mapping approaches with large sets of environmental covariates. SOIL, 4(1), 1–22. https://doi.org/10.5194/soil-4-1-2018\n\n\nNussbaum, M., Walthert, L., Fraefel, M., Greiner, L., & Papritz, A. (2017). Mapping of soil properties at high resolution in Switzerland using boosted geoadditive models. SOIL, 3(4), 191–210. https://doi.org/10.5194/soil-3-191-2017"
  },
  {
    "objectID": "02-data_preparation.html#load-data",
    "href": "02-data_preparation.html#load-data",
    "title": "2  Data preparation",
    "section": "2.1 Load data",
    "text": "2.1 Load data\nThe data required for ditigal soil mapping are:\n\nSoil samples of variables measured in the field. These observations need to be geo-located and are used as the target for the model that is then used for spatial upscaling.\nEnvironmental covariates, provided as geospatial raster maps. These covariates act as predictors in the model that is used for spatial upscaling.\n\nLet’s load them\n\n2.1.1 Soil samples\n\n\nCode\n# Load soil data from sampling locations\ndf_obs &lt;- readr::read_csv(\n  here::here(\"data-raw/soildata/berne_soil_sampling_locations.csv\")\n  )\n\n# Display data\nhead(df_obs) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsite_id_unique\ntimeset\nx\ny\ndataset\ndclass\nwaterlog.30\nwaterlog.50\nwaterlog.100\nph.0.10\nph.10.30\nph.30.50\nph.50.100\n\n\n\n\n4_26-In-005\nd1968_1974_ptf\n2571994\n1203001\nvalidation\npoor\n0\n0\n1\n6.071733\n6.227780\n7.109235\n7.214589\n\n\n4_26-In-006\nd1974_1978\n2572149\n1202965\ncalibration\npoor\n0\n1\n1\n6.900000\n6.947128\n7.203502\n7.700000\n\n\n4_26-In-012\nd1974_1978\n2572937\n1203693\ncalibration\nmoderate\n0\n1\n1\n6.200000\n6.147128\n5.603502\n5.904355\n\n\n4_26-In-014\nd1974_1978\n2573374\n1203710\nvalidation\nwell\n0\n0\n0\n6.600000\n6.754607\n7.200000\n7.151129\n\n\n4_26-In-015\nd1968_1974_ptf\n2573553\n1203935\nvalidation\nmoderate\n0\n0\n1\n6.272715\n6.272715\n6.718392\n7.269008\n\n\n4_26-In-016\nd1968_1974_ptf\n2573310\n1204328\ncalibration\npoor\n0\n0\n1\n6.272715\n6.160700\n5.559031\n5.161655\n\n\n\n\n\nThe dataset on soil samples from Bern holds 13 variables for 1052 entries (more information here):\n\nsite_id_unique: The location’s unique site id.\ntimeset: The sampling year and information on sampling type for soil pH (no label: CaCl\\(_2\\) laboratory measurement, field: indicator solution used in field, ptf: H\\(_2\\)O laboratory measurement transferred by pedotransfer function).\nx: The x (easting) coordinates in meters following the (CH1903/LV03) system.\ny: The y (northing) coordinates in meters following the (CH1903/LV03) system.\ndataset: Specification whether a sample is used for model training (\"calibration\") or testing (\"validation\") (this is based on randomization to ensure even spatial coverage).\ndclass: Soil drainage class\nwaterlog.30, waterlog.50, waterlog.100: Specification whether soil was water logged at 30, 50, or 100 cm depth (0 = No, 1 = Yes).\nph.0.10, ph.10.30, ph.30.50, ph.50.100: Average soil pH between 0-10, 10-30, 30-50, and 50-100 cm depth.\n\n\n\n2.1.2 Environmental covariates\nNow, let’s load the covariates that we want to produce our soil maps with. These files are in the geoTIFF format - geolocated TIFF files.\n\n\nCode\n# Get a list with the path to all raster files\nlist_raster &lt;- list.files(\n  here::here(\"data-raw/geodata/covariates/\"),\n  full.names = TRUE\n  )\n\n# Display data (lapply to clean names)\nlapply(\n  list_raster, \n  function(x) sub(\".*/(.*)\", \"\\\\1\", x)\n  ) |&gt; \n  unlist() |&gt; \n  head(5) |&gt; \n  print()\n\n\n[1] \"NegO.tif\"         \"PosO.tif\"         \"Se_MRRTF2m.tif\"   \"Se_MRVBF2m.tif\"  \n[5] \"Se_NO2m_r500.tif\"\n\n\nThe output above shows the first five raster files with rather cryptic names. The meaning of all 91 raster files are given in Chapter 6. Make sure to have a look at that list as it will help you to interpret your model results later on. Let’s look at one of these raster files, Se_slope2m.tif, to get a better understanding for our data. That file contains the local slope of the terrain, derived from a digital elevation model with 2 m resolution:\n\n\nCode\n# Load a raster file as example: Picking the slope profile at 2 m resolution\nraster_example &lt;- terra::rast(\n  here::here(\"data-raw/geodata/covariates/Se_slope2m.tif\")\n  )\nraster_example\n\n\nclass       : SpatRaster \ndimensions  : 986, 2428, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : 2568140, 2616700, 1200740, 1220460  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903+ / LV95 \nsource      : Se_slope2m.tif \nname        : Se_slope2m \nmin value   :    0.00000 \nmax value   :   85.11286 \n\n\nAs shown in the output, a raster object has the following properties (among others, see ?terra::rast):\n\nclass: The class of the file, here a SpatRaster.\ndimensions: The number of rows, columns, years (if temporal encoding).\nresolution: The resolution of the coordinate system, here it is 20 in both axes.\nextent: The extent of the coordinate system defined by min and max values on the x and y axes.\ncoord. ref.: Reference coordinate system. Here, the raster is encoded using the LV95 geodetic reference system from which the projected coordinate system CH1903+ is derived.\nsource: The name of the source file.\nnames: The name of the raster file (mostly the file name without file-specific ending)\nmin value: The lowest value of all cells.\nmax value: The highest value of all cells.\n\n\n\n\n\n\n\nTip\n\n\n\nThe code chunks filtered for a random sub-sample of 15 variables. As described in Chapter 5, your task will be to investigate all covariates and find the ones that can best be used for your modelling task."
  },
  {
    "objectID": "02-data_preparation.html#visualise-data",
    "href": "02-data_preparation.html#visualise-data",
    "title": "2  Data preparation",
    "section": "2.2 Visualise data",
    "text": "2.2 Visualise data\nNow, let’s look at a visualisation of this raster file. Since we have selected the slope at 2 m resolution, we expect a relief-like map with a color gradient that indicates the steepness of the terrain. A quick way to look at a raster object is to use the generic plot() function.\n\n\nCode\n# Plot raster example\nterra::plot(raster_example)\n\n\n\n\n\nTo have more flexibility with visualising the data, we can use the ggplot() in combination with the {tidyterra} package.\n\n\nCode\nlibrary(tidyterra)\n\n# To have some more flexibility, we can plot this in the ggplot-style as such:\nggplot2::ggplot() +\n  tidyterra::geom_spatraster(data = raster_example) +\n  ggplot2::scale_fill_viridis_c(\n    na.value = NA,\n    option = \"magma\",\n    name = \"Slope (%) \\n\"\n    ) +\n  ggplot2::theme_bw() +\n  ggplot2::scale_x_continuous(expand = c(0, 0)) +  # avoid gap between plotting area and axis\n  ggplot2::scale_y_continuous(expand = c(0, 0)) +\n  ggplot2::labs(title = \"Slope of the Study Area\")\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the second plot has different coordinates than the upper one. That is because the data was automatically projected to the World Geodetic System (WGS84, ESPG: 4326).\n\n\nThis looks already interesting but we can put our data into a bit more context. For example, a larger map background would be useful to get a better orientation of our location. Also, it would be nice to see where our sampling locations are and to differentiate these locations by whether they are part of the training or testing dataset. Bringing this all together requires some more understanding of plotting maps in R. So, don’t worry if you do not understand everything in the code chunk below and enjoy the visualizations:\n\n\nCode\n# To get our map working correctly, we have to ensure that all the input data\n# is in the same coordinate system. Since our Bern data is in the Swiss \n# coordinate system, we have to transform the sampling locations to the \n# World Geodetic System first.\n# To look up EPSG Codes: https://epsg.io/\n# World Geodetic System 1984:  4326\n# Swiss CH1903+ / LV95: 2056\n\n# For the raster:\nrasta &lt;- terra::project(raster_example, \"+init=EPSG:4326\")\n\n# Let's make a function for transforming the sampling locations:\nchange_coords &lt;- function(data, from_CRS, to_CRS) {\n  \n  # Check if data input is correct\n  if (!all(names(data) %in% c(\"id\", \"lat\", \"lon\"))) {\n    stop(\"Input data needs variables: id, lat, lon\")\n  }\n  \n  # Create simple feature for old CRS\n  sf_old_crs &lt;- sf::st_as_sf(data, coords = c(\"lon\", \"lat\"), crs = from_CRS)\n  \n  # Transform to new CRS\n  sf_new_crs     &lt;- sf::st_transform(sf_old_crs, crs = to_CRS)\n  sf_new_crs$lat &lt;- sf::st_coordinates(sf_new_crs)[, \"Y\"]\n  sf_new_crs$lon &lt;- sf::st_coordinates(sf_new_crs)[, \"X\"]\n  \n  sf_new_crs &lt;- sf_new_crs |&gt; dplyr::as_tibble() |&gt; dplyr::select(id, lat, lon)\n  \n  # Return new CRS\n  return(sf_new_crs)\n}\n\n# Transform dataframes\ncoord_train &lt;- df_obs |&gt; \n  dplyr::filter(dataset == \"calibration\") |&gt; \n  dplyr::select(site_id_unique, x, y) |&gt; \n  dplyr::rename(id = site_id_unique, lon = x, lat = y) |&gt; \n  change_coords(\n    from_CRS = 2056, \n    to_CRS = 4326\n    )\n\ncoord_test &lt;- df_obs |&gt; \n  dplyr::filter(dataset == \"validation\") |&gt; \n  dplyr::select(site_id_unique, x, y) |&gt; \n  dplyr::rename(id = site_id_unique, lon = x, lat = y) |&gt; \n  change_coords(\n    from_CRS = 2056, \n    to_CRS = 4326\n    )\n\n\n\n\nCode\n# Notes: \n# - This code may only work when installing the development branch of {leaflet}:\n# remotes::install_github('rstudio/leaflet')\n# - You might have to do library(terra) for R to find functions needed in the backend\nlibrary(terra)\n\n# Let's get a nice color palette now for easy reference\npal &lt;- leaflet::colorNumeric(\n  \"magma\",\n  terra::values(r),\n  na.color = \"transparent\"\n  )\n\n# Next, we build a leaflet map\nleaflet::leaflet() |&gt; \n  # As base maps, use two provided by ESRI\n  leaflet::addProviderTiles(leaflet::providers$Esri.WorldImagery, group = \"World Imagery\") |&gt;\n  leaflet::addProviderTiles(leaflet::providers$Esri.WorldTopoMap, group = \"World Topo\") |&gt;\n  # Add our raster file\n  leaflet::addRasterImage(\n    rasta,\n    colors = pal,\n    opacity = 0.6,\n    group = \"raster\"\n    ) |&gt;\n  # Add markers for sampling locations\n  leaflet::addCircleMarkers(\n    data = coord_train,\n    lng = ~lon,  # Column name for x coordinates\n    lat = ~lat,  # Column name for y coordinates\n    group = \"training\",\n    color = \"black\"\n  ) |&gt;\n    leaflet::addCircleMarkers(\n    data = coord_test,\n    lng = ~lon,  # Column name for x coordinates\n    lat = ~lat,  # Column name for y coordinates\n    group = \"validation\",\n    color = \"red\"\n  ) |&gt;\n  # Add some layout and legend\n  leaflet::addLayersControl(\n    baseGroups = c(\"World Imagery\",\"World Topo\"),\n    position = \"topleft\",\n    options = leaflet::layersControlOptions(collapsed = FALSE),\n    overlayGroups = c(\"raster\", \"training\", \"validation\")\n    ) |&gt;\n  leaflet::addLegend(\n    pal = pal,\n    values = terra::values(r),\n    title = \"Slope (%)\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plotting example is based to the one shown in the AGDS 2 tutorial “Handful of Pixels” on phenology. More information on using spatial data in R can be found there in the Chapter on Geospatial data in R.\n\n\nThat looks great! At a first glance, it is a bit crowded but once you zoom in, you can investigate our study area quite nicely. You can check whether the slope raster file makes sense by comparing it against the base maps. Can you see how cliffs along the Aare river, hills, and even gravel quarries show high slope values. We also see that our testing dataset is randomly distributed across the area covered by the training dataset."
  },
  {
    "objectID": "02-data_preparation.html#combine-data",
    "href": "02-data_preparation.html#combine-data",
    "title": "2  Data preparation",
    "section": "2.3 Combine data",
    "text": "2.3 Combine data\nNow that we have played with a few visualizations, let’s get back to preparing our data. The {terra} package comes with the very useful tool to stack multiple rasters on top of each other if they share the spatial grid (extent and resolution). To do so, we just have to feed in the vector of file names list_raster:\n\n\nCode\n# Load all files as one batch\nall_rasters &lt;- terra::rast(list_raster)\nall_rasters\n\n\nclass       : SpatRaster \ndimensions  : 986, 2428, 91  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : 2568140, 2616700, 1200740, 1220460  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903+ / LV95 \nsources     : NegO.tif  \n              PosO.tif  \n              Se_MRRTF2m.tif  \n              ... and 88 more source(s)\nnames       :      NegO,      PosO, Se_MRRTF2m, Se_MRVBF2m, Se_NO2m_r500, Se_PO2m_r500, ... \nmin values  : 0.8109335, 0.8742412,   0.000000,   0.000000,    0.2755551,    0.3541574, ... \nmax values  : 1.5921584, 1.6218545,   6.965698,   7.991423,    1.6376855,    1.6430260, ... \n\n\nNote that above, we have stacked only a random of all available raster data (list_raster) which we have generated previously.\nNow, we do not want to have the covariates’ data from all cells in the raster file. Rather, we want to reduce our stacked rasters to the x and y coordinates for which we have soil sampling data. We can do this using the terra::extract() function. Then, we want to merge the two dataframes of soil data and covariates data by their coordinates. Since number of rows and the order of the covariate data is the same as the “Bern data” (soil samples), we can simply bind their columns with cbind():\n\n\nCode\n# Extract coordinates from sampling locations\nsampling_xy &lt;- df_obs |&gt; \n  dplyr::select(x, y)\n\n# From all rasters, extract values for sampling coordinates\ndf_covars &lt;- terra::extract(\n  all_rasters,  # The raster we want to extract from\n  sampling_xy,  # A matrix of x and y values to extract for\n  ID = FALSE    # To not add a default ID column to the output\n  )\n\ndf_full &lt;- cbind(df_obs, df_covars)\nhead(df_full) |&gt; \n  knitr::kable() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsite_id_unique\ntimeset\nx\ny\ndataset\ndclass\nwaterlog.30\nwaterlog.50\nwaterlog.100\nph.0.10\nph.10.30\nph.30.50\nph.50.100\nNegO\nPosO\nSe_MRRTF2m\nSe_MRVBF2m\nSe_NO2m_r500\nSe_PO2m_r500\nSe_SAR2m\nSe_SCA2m\nSe_TWI2m\nSe_TWI2m_s15\nSe_TWI2m_s60\nSe_alti2m_std_50c\nSe_conv2m\nSe_curv25m\nSe_curv2m\nSe_curv2m_fmean_50c\nSe_curv2m_fmean_5c\nSe_curv2m_s60\nSe_curv2m_std_50c\nSe_curv2m_std_5c\nSe_curv50m\nSe_curv6m\nSe_curvplan25m\nSe_curvplan2m\nSe_curvplan2m_fmean_50c\nSe_curvplan2m_fmean_5c\nSe_curvplan2m_s60\nSe_curvplan2m_s7\nSe_curvplan2m_std_50c\nSe_curvplan2m_std_5c\nSe_curvplan50m\nSe_curvprof25m\nSe_curvprof2m\nSe_curvprof2m_fmean_50c\nSe_curvprof2m_fmean_5c\nSe_curvprof2m_s60\nSe_curvprof2m_s7\nSe_curvprof2m_std_50c\nSe_curvprof2m_std_5c\nSe_curvprof50m\nSe_diss2m_50c\nSe_diss2m_5c\nSe_e_aspect25m\nSe_e_aspect2m\nSe_e_aspect2m_5c\nSe_e_aspect50m\nSe_n_aspect2m\nSe_n_aspect2m_50c\nSe_n_aspect2m_5c\nSe_n_aspect50m\nSe_n_aspect6m\nSe_rough2m_10c\nSe_rough2m_5c\nSe_rough2m_rect3c\nSe_slope2m\nSe_slope2m_fmean_50c\nSe_slope2m_fmean_5c\nSe_slope2m_s60\nSe_slope2m_s7\nSe_slope2m_std_50c\nSe_slope2m_std_5c\nSe_slope50m\nSe_slope6m\nSe_toposcale2m_r3_r50_i10s\nSe_tpi_2m_50c\nSe_tpi_2m_5c\nSe_tri2m_altern_3c\nSe_tsc10_2m\nSe_vrm2m\nSe_vrm2m_r10c\nbe_gwn25_hdist\nbe_gwn25_vdist\ncindx10_25\ncindx50_25\ngeo500h1id\ngeo500h3id\nlgm\nlsf\nmrrtf25\nmrvbf25\nmt_gh_y\nmt_rr_y\nmt_td_y\nmt_tt_y\nmt_ttvar\nprotindx\nterrTextur\ntsc25_18\ntsc25_40\nvdcn25\nvszone\n\n\n\n\n4_26-In-005\nd1968_1974_ptf\n2571994\n1203001\nvalidation\npoor\n0\n0\n1\n6.071733\n6.227780\n7.109235\n7.214589\n1.569110\n1.534734\n5.930607\n6.950892\n1.562085\n1.548762\n4.000910\n16.248077\n0.0011592\n0.0032796\n0.0049392\n0.3480562\n-40.5395088\n-0.0014441\n-1.9364884\n-0.0062570\n0.0175912\n0.0002296\n2.9204133\n1.1769447\n0.0031319\n-0.5886537\n-0.0042508\n-1.0857303\n-0.0445323\n-0.0481024\n-0.0504083\n-0.1655090\n1.5687343\n0.6229440\n0.0007920\n-0.0028067\n0.8507581\n-0.0382753\n-0.0656936\n-0.0506380\n-0.0732220\n1.6507173\n0.7082230\n-0.0023399\n0.3934371\n0.1770810\n-0.9702092\n-0.5661940\n-0.7929600\n-0.9939429\n-0.2402939\n-0.2840056\n-0.6084610\n-0.0577110\n-0.7661251\n0.3228087\n0.2241062\n0.2003846\n1.1250136\n0.9428899\n0.6683306\n0.9333237\n0.7310556\n0.8815832\n0.3113754\n0.3783818\n0.5250366\n0\n-0.0940372\n-0.0583917\n10.319408\n0.4645128\n0.0002450\n0.000125\n234.39087\n1.2986320\n-10.62191\n-6.9658718\n6\n0\n7\n0.0770846\n0.0184651\n4.977099\n1316.922\n9931.120\n58\n98\n183\n0.0159717\n0.6248673\n0.3332805\n1.784737\n65.62196\n6\n\n\n4_26-In-006\nd1974_1978\n2572149\n1202965\ncalibration\npoor\n0\n1\n1\n6.900000\n6.947128\n7.203502\n7.700000\n1.568917\n1.533827\n5.984921\n6.984581\n1.543384\n1.558683\n4.001326\n3.357315\n0.0139006\n0.0070509\n0.0067992\n0.1484705\n19.0945148\n-0.0190294\n2.1377332\n0.0021045\n0.0221433\n0.0000390\n3.8783867\n4.3162045\n-0.0171786\n0.1278165\n-0.0119618\n-0.3522736\n-0.0501855\n-0.3270764\n-0.1004921\n-0.5133076\n2.0736780\n2.2502327\n-0.0073879\n0.0070676\n-2.4900069\n-0.0522900\n-0.3492197\n-0.1005311\n-0.4981292\n2.1899190\n2.4300070\n0.0097907\n0.4014700\n0.7360508\n0.5683194\n-0.3505180\n0.8753148\n0.3406741\n0.4917848\n-0.5732749\n0.4801802\n-0.4550385\n0.7722272\n0.2730940\n0.2489859\n0.2376962\n1.3587183\n1.0895698\n0.9857153\n1.0231543\n1.0398037\n1.0152543\n0.5357812\n0.0645478\n0.5793087\n0\n-0.0014692\n0.0180000\n12.603136\n0.5536283\n0.0005389\n0.000300\n127.41681\n1.7064546\n-10.87862\n-11.8201790\n6\n0\n7\n0.0860347\n0.0544361\n4.975796\n1317.000\n9931.672\n58\n98\n183\n0.0204794\n0.7573612\n0.3395441\n1.832904\n69.16074\n6\n\n\n4_26-In-012\nd1974_1978\n2572937\n1203693\ncalibration\nmoderate\n0\n1\n1\n6.200000\n6.147128\n5.603502\n5.904355\n1.569093\n1.543057\n5.953919\n6.990917\n1.565405\n1.563151\n4.000320\n11.330072\n0.0011398\n0.0021498\n0.0017847\n0.1112066\n-9.1396294\n0.0039732\n-0.4178924\n0.0009509\n0.0431735\n0.0034232\n0.7022317\n0.4170935\n-0.0026431\n-0.0183221\n0.0015183\n-0.2168447\n-0.0079620\n0.0053904\n-0.0091239\n-0.0110896\n0.3974485\n0.2292406\n-0.0013561\n-0.0024548\n0.2010477\n-0.0089129\n-0.0377831\n-0.0125471\n-0.0052359\n0.4158890\n0.2700820\n0.0012870\n0.6717541\n0.4404107\n-0.6987815\n-0.1960597\n-0.3866692\n-0.7592779\n-0.9633239\n-0.3006475\n-0.9221049\n-0.3257418\n-0.9502072\n0.2305476\n0.2182523\n0.1434273\n0.7160403\n0.5758902\n0.5300468\n0.5107915\n0.5744110\n0.4975456\n0.2001768\n0.1311051\n0.4620202\n0\n0.0340407\n-0.0145804\n7.100000\n0.4850160\n0.0000124\n0.000000\n143.41533\n0.9372618\n22.10210\n0.2093917\n6\n0\n7\n0.0737963\n3.6830916\n4.986864\n1315.134\n9935.438\n58\n98\n183\n0.0048880\n0.7978453\n0.4455501\n1.981526\n63.57096\n6\n\n\n4_26-In-014\nd1974_1978\n2573374\n1203710\nvalidation\nwell\n0\n0\n0\n6.600000\n6.754607\n7.200000\n7.151129\n1.569213\n1.542792\n4.856076\n6.964162\n1.562499\n1.562670\n4.000438\n42.167496\n0.0000000\n0.0008454\n0.0021042\n0.3710849\n-0.9318936\n-0.0371234\n-0.0289909\n0.0029348\n-0.1056513\n0.0127788\n1.5150748\n0.2413423\n0.0020990\n-0.0706228\n-0.0113604\n-0.0272214\n-0.0301961\n-0.0346193\n-0.0273140\n-0.0343277\n0.8245047\n0.1029889\n-0.0041158\n0.0257630\n0.0017695\n-0.0331309\n0.0710320\n-0.0400928\n0.0529446\n0.8635767\n0.1616543\n-0.0062147\n0.4988544\n0.4217250\n-0.8485889\n-0.8836724\n-0.8657616\n-0.8993938\n-0.4677161\n-0.5735765\n-0.4998477\n-0.4121092\n-0.4782534\n0.3859352\n0.2732429\n0.1554769\n0.8482135\n0.8873205\n0.8635756\n0.9015982\n0.8518201\n0.5767300\n0.2149791\n0.3928713\n0.8432562\n0\n0.0686932\n-0.0085602\n8.303085\n0.3951114\n0.0000857\n0.000100\n165.80418\n0.7653937\n-20.11569\n-7.7729993\n6\n0\n7\n0.0859686\n0.0075817\n5.285522\n1315.160\n9939.923\n58\n98\n183\n0.0064054\n0.4829135\n0.4483251\n2.113142\n64.60535\n6\n\n\n4_26-In-015\nd1968_1974_ptf\n2573553\n1203935\nvalidation\nmoderate\n0\n0\n1\n6.272715\n6.272715\n6.718392\n7.269008\n1.570359\n1.541979\n4.130917\n6.945287\n1.550528\n1.562685\n4.000948\n5.479310\n0.0054557\n0.0043268\n0.0045225\n0.3907509\n4.2692256\n0.0378648\n0.6409346\n0.0022611\n-0.1020419\n0.0161510\n3.6032522\n1.8169731\n0.0346340\n0.0476020\n0.0378154\n0.2968794\n-0.0179657\n-0.0137853\n-0.0146946\n0.0060875\n1.4667766\n0.9816071\n0.0337645\n-0.0000494\n-0.3440553\n-0.0202268\n0.0882566\n-0.0308456\n0.0929077\n2.6904552\n1.0218329\n-0.0008695\n0.6999696\n0.3944107\n-0.8918364\n-0.7795515\n-0.8864348\n-0.4249992\n0.5919228\n0.4304937\n0.4614536\n0.6559467\n0.4574654\n0.4330348\n0.3299487\n0.1889674\n1.2301254\n1.8937486\n1.2098556\n1.5986075\n1.2745584\n2.7759163\n0.5375320\n0.3582314\n1.1426100\n0\n0.3005829\n0.0061576\n10.110727\n0.5134069\n0.0002062\n0.000200\n61.39244\n1.0676192\n-55.12566\n-14.0670462\n6\n0\n7\n0.0650000\n0.0007469\n5.894688\n1315.056\n9942.032\n58\n98\n183\n0.0042235\n0.6290755\n0.3974232\n2.080674\n61.16533\n6\n\n\n4_26-In-016\nd1968_1974_ptf\n2573310\n1204328\ncalibration\npoor\n0\n0\n1\n6.272715\n6.160700\n5.559031\n5.161655\n1.569434\n1.541606\n2.030315\n6.990967\n1.563066\n1.552568\n4.000725\n13.499996\n0.0000000\n0.0001476\n0.0003817\n0.1931891\n-0.1732794\n-0.1602274\n0.0318570\n-0.0035833\n-0.1282881\n0.0003549\n1.5897882\n0.8171870\n-0.0123340\n0.0400775\n-0.0813964\n0.0100844\n-0.0049875\n0.0320331\n-0.0049053\n0.0374298\n0.7912259\n0.3455668\n-0.0059622\n0.0788309\n-0.0217726\n-0.0014042\n0.1603212\n-0.0052602\n0.0867119\n1.0207798\n0.6147888\n0.0063718\n0.3157751\n0.5292308\n-0.8766075\n0.8129975\n0.5905659\n0.1640853\n0.5820994\n0.6325440\n0.8054439\n0.7448481\n0.6081498\n0.3688371\n0.2607146\n0.1763995\n1.0906221\n1.0418727\n0.8515157\n1.2106605\n0.8916541\n1.2163279\n0.4894866\n0.2049688\n0.7156029\n0\n-0.0910767\n0.0034276\n9.574804\n0.3864355\n0.0001151\n0.000525\n310.05014\n0.1321367\n-17.16055\n-28.0693741\n6\n0\n7\n0.0731646\n0.0128017\n5.938320\n1315.000\n9940.597\n58\n98\n183\n0.0040683\n0.6997021\n0.4278295\n2.041467\n55.78354\n6"
  },
  {
    "objectID": "02-data_preparation.html#more-data-wrangling",
    "href": "02-data_preparation.html#more-data-wrangling",
    "title": "2  Data preparation",
    "section": "2.4 More data wrangling",
    "text": "2.4 More data wrangling\nNow, not all our covariates may be continuous variables and therefore have to be encoded as factors. As an easy check, we can take the original corvariates data and check for the number of unique values in each raster. If the variable is continuous, we expect that there are a lot of different values - at maximum 1052 different values because we have that many entries. So, let’s have a look and assume that variables with 10 or less different values are categorical variables.\n\n\nCode\nvars_categorical &lt;- df_covars |&gt; \n  \n  # Get number of distinct values per variable\n  dplyr::summarise(dplyr::across(dplyr::everything(), ~dplyr::n_distinct(.))) |&gt; \n  \n  # Turn df into long format for easy filtering\n  tidyr::pivot_longer(\n    dplyr::everything(), \n    names_to = \"variable\", \n    values_to = \"n\"\n    ) |&gt; \n  \n  # Filter out variables with 10 or less distinct values\n  dplyr::filter(n &lt;= 10) |&gt;\n  \n  # Extract the names of these variables\n  dplyr::pull('variable')\n\ncat(\"Variables with less than 10 distinct values:\", \n    ifelse(length(vars_categorical) == 0, \"none\", vars_categorical))\n\n\nVariables with less than 10 distinct values: geo500h1id\n\n\nNow that we have the names of the categorical values, we can mutate these columns in our data frame using the base function as.factor():\n\n\nCode\ndf_full &lt;- df_full |&gt; \n  dplyr::mutate(dplyr::across(all_of(vars_categorical), ~as.factor(.)))"
  },
  {
    "objectID": "02-data_preparation.html#checking-missing-data",
    "href": "02-data_preparation.html#checking-missing-data",
    "title": "2  Data preparation",
    "section": "2.5 Checking missing data",
    "text": "2.5 Checking missing data\nWe are almost done with our data preparation, we just need to reduce it to sampling locations for which we have a decent amount of data on the covariates. Else, we blow up the model calibration with data that is not informative enough.\n\n\nCode\n# Get number of rows to calculate percentages\nn_rows &lt;- nrow(df_full)\n\n# Get number of distinct values per variable\ndf_full |&gt; \n  dplyr::summarise(dplyr::across(dplyr::everything(), \n                                 ~ length(.) - sum(is.na(.)))) |&gt; \n  tidyr::pivot_longer(dplyr::everything(), \n                      names_to = \"variable\", \n                      values_to = \"n\") |&gt;\n  dplyr::mutate(perc_available = round(n / n_rows * 100)) |&gt; \n  dplyr::arrange(perc_available) |&gt; \n  head(10) |&gt; \n  knitr::kable()\n\n\n\n\n\nvariable\nn\nperc_available\n\n\n\n\nph.30.50\n856\n81\n\n\nph.10.30\n866\n82\n\n\nph.50.100\n859\n82\n\n\ntimeset\n871\n83\n\n\nph.0.10\n870\n83\n\n\ndclass\n1006\n96\n\n\nsite_id_unique\n1052\n100\n\n\nx\n1052\n100\n\n\ny\n1052\n100\n\n\ndataset\n1052\n100\n\n\n\n\n\nThis looks good, we have no variable with a substantial amount of missing data. Generally, only pH measurements are lacking, which we should keep in mind when making predictions and inferences. Another great way to explore your data, is using the {visdat} package:\n\n\nCode\ndf_full |&gt; \n  dplyr::select(1:20) |&gt;   # reduce data for readability of the plot\n  visdat::vis_miss()\n\n\n\n\n\nAlright, we see that we are not missing any data in the covariate data. Mostly sampled data, specifically pH and timeset data is missing. We also see that this missing data is mostly from the same entries, so if we keep only entries where we have pH data - which is what we are interested in here - we have a dataset with pracitally no missing data."
  },
  {
    "objectID": "02-data_preparation.html#save-data",
    "href": "02-data_preparation.html#save-data",
    "title": "2  Data preparation",
    "section": "2.6 Save data",
    "text": "2.6 Save data\n\n\nCode\nif (!dir.exists(here::here(\"data\"))) system(paste0(\"mkdir \", here::here(\"data\")))\nsaveRDS(df_full, \n        here::here(\"data/df_full.rds\"))"
  },
  {
    "objectID": "03-model_training.html#load-data",
    "href": "03-model_training.html#load-data",
    "title": "3  Train a Random Forest",
    "section": "3.1 Load data",
    "text": "3.1 Load data\nIn the previous Chapter, we created a dataframe that holds information on the soil sampling locations and the covariates that we extracted for these positions. Let’s load this datafarme into our environment\n\n\nCode\ndf_full &lt;- readRDS(here::here(\"data/df_full.rds\"))\n\nhead(df_full) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsite_id_unique\ntimeset\nx\ny\ndataset\ndclass\nwaterlog.30\nwaterlog.50\nwaterlog.100\nph.0.10\nph.10.30\nph.30.50\nph.50.100\nNegO\nPosO\nSe_MRRTF2m\nSe_MRVBF2m\nSe_NO2m_r500\nSe_PO2m_r500\nSe_SAR2m\nSe_SCA2m\nSe_TWI2m\nSe_TWI2m_s15\nSe_TWI2m_s60\nSe_alti2m_std_50c\nSe_conv2m\nSe_curv25m\nSe_curv2m\nSe_curv2m_fmean_50c\nSe_curv2m_fmean_5c\nSe_curv2m_s60\nSe_curv2m_std_50c\nSe_curv2m_std_5c\nSe_curv50m\nSe_curv6m\nSe_curvplan25m\nSe_curvplan2m\nSe_curvplan2m_fmean_50c\nSe_curvplan2m_fmean_5c\nSe_curvplan2m_s60\nSe_curvplan2m_s7\nSe_curvplan2m_std_50c\nSe_curvplan2m_std_5c\nSe_curvplan50m\nSe_curvprof25m\nSe_curvprof2m\nSe_curvprof2m_fmean_50c\nSe_curvprof2m_fmean_5c\nSe_curvprof2m_s60\nSe_curvprof2m_s7\nSe_curvprof2m_std_50c\nSe_curvprof2m_std_5c\nSe_curvprof50m\nSe_diss2m_50c\nSe_diss2m_5c\nSe_e_aspect25m\nSe_e_aspect2m\nSe_e_aspect2m_5c\nSe_e_aspect50m\nSe_n_aspect2m\nSe_n_aspect2m_50c\nSe_n_aspect2m_5c\nSe_n_aspect50m\nSe_n_aspect6m\nSe_rough2m_10c\nSe_rough2m_5c\nSe_rough2m_rect3c\nSe_slope2m\nSe_slope2m_fmean_50c\nSe_slope2m_fmean_5c\nSe_slope2m_s60\nSe_slope2m_s7\nSe_slope2m_std_50c\nSe_slope2m_std_5c\nSe_slope50m\nSe_slope6m\nSe_toposcale2m_r3_r50_i10s\nSe_tpi_2m_50c\nSe_tpi_2m_5c\nSe_tri2m_altern_3c\nSe_tsc10_2m\nSe_vrm2m\nSe_vrm2m_r10c\nbe_gwn25_hdist\nbe_gwn25_vdist\ncindx10_25\ncindx50_25\ngeo500h1id\ngeo500h3id\nlgm\nlsf\nmrrtf25\nmrvbf25\nmt_gh_y\nmt_rr_y\nmt_td_y\nmt_tt_y\nmt_ttvar\nprotindx\nterrTextur\ntsc25_18\ntsc25_40\nvdcn25\nvszone\n\n\n\n\n4_26-In-005\nd1968_1974_ptf\n2571994\n1203001\nvalidation\npoor\n0\n0\n1\n6.071733\n6.227780\n7.109235\n7.214589\n1.569110\n1.534734\n5.930607\n6.950892\n1.562085\n1.548762\n4.000910\n16.248077\n0.0011592\n0.0032796\n0.0049392\n0.3480562\n-40.5395088\n-0.0014441\n-1.9364884\n-0.0062570\n0.0175912\n0.0002296\n2.9204133\n1.1769447\n0.0031319\n-0.5886537\n-0.0042508\n-1.0857303\n-0.0445323\n-0.0481024\n-0.0504083\n-0.1655090\n1.5687343\n0.6229440\n0.0007920\n-0.0028067\n0.8507581\n-0.0382753\n-0.0656936\n-0.0506380\n-0.0732220\n1.6507173\n0.7082230\n-0.0023399\n0.3934371\n0.1770810\n-0.9702092\n-0.5661940\n-0.7929600\n-0.9939429\n-0.2402939\n-0.2840056\n-0.6084610\n-0.0577110\n-0.7661251\n0.3228087\n0.2241062\n0.2003846\n1.1250136\n0.9428899\n0.6683306\n0.9333237\n0.7310556\n0.8815832\n0.3113754\n0.3783818\n0.5250366\n0\n-0.0940372\n-0.0583917\n10.319408\n0.4645128\n0.0002450\n0.000125\n234.39087\n1.2986320\n-10.62191\n-6.9658718\n6\n0\n7\n0.0770846\n0.0184651\n4.977099\n1316.922\n9931.120\n58\n98\n183\n0.0159717\n0.6248673\n0.3332805\n1.784737\n65.62196\n6\n\n\n4_26-In-006\nd1974_1978\n2572149\n1202965\ncalibration\npoor\n0\n1\n1\n6.900000\n6.947128\n7.203502\n7.700000\n1.568917\n1.533827\n5.984921\n6.984581\n1.543384\n1.558683\n4.001326\n3.357315\n0.0139006\n0.0070509\n0.0067992\n0.1484705\n19.0945148\n-0.0190294\n2.1377332\n0.0021045\n0.0221433\n0.0000390\n3.8783867\n4.3162045\n-0.0171786\n0.1278165\n-0.0119618\n-0.3522736\n-0.0501855\n-0.3270764\n-0.1004921\n-0.5133076\n2.0736780\n2.2502327\n-0.0073879\n0.0070676\n-2.4900069\n-0.0522900\n-0.3492197\n-0.1005311\n-0.4981292\n2.1899190\n2.4300070\n0.0097907\n0.4014700\n0.7360508\n0.5683194\n-0.3505180\n0.8753148\n0.3406741\n0.4917848\n-0.5732749\n0.4801802\n-0.4550385\n0.7722272\n0.2730940\n0.2489859\n0.2376962\n1.3587183\n1.0895698\n0.9857153\n1.0231543\n1.0398037\n1.0152543\n0.5357812\n0.0645478\n0.5793087\n0\n-0.0014692\n0.0180000\n12.603136\n0.5536283\n0.0005389\n0.000300\n127.41681\n1.7064546\n-10.87862\n-11.8201790\n6\n0\n7\n0.0860347\n0.0544361\n4.975796\n1317.000\n9931.672\n58\n98\n183\n0.0204794\n0.7573612\n0.3395441\n1.832904\n69.16074\n6\n\n\n4_26-In-012\nd1974_1978\n2572937\n1203693\ncalibration\nmoderate\n0\n1\n1\n6.200000\n6.147128\n5.603502\n5.904355\n1.569093\n1.543057\n5.953919\n6.990917\n1.565405\n1.563151\n4.000320\n11.330072\n0.0011398\n0.0021498\n0.0017847\n0.1112066\n-9.1396294\n0.0039732\n-0.4178924\n0.0009509\n0.0431735\n0.0034232\n0.7022317\n0.4170935\n-0.0026431\n-0.0183221\n0.0015183\n-0.2168447\n-0.0079620\n0.0053904\n-0.0091239\n-0.0110896\n0.3974485\n0.2292406\n-0.0013561\n-0.0024548\n0.2010477\n-0.0089129\n-0.0377831\n-0.0125471\n-0.0052359\n0.4158890\n0.2700820\n0.0012870\n0.6717541\n0.4404107\n-0.6987815\n-0.1960597\n-0.3866692\n-0.7592779\n-0.9633239\n-0.3006475\n-0.9221049\n-0.3257418\n-0.9502072\n0.2305476\n0.2182523\n0.1434273\n0.7160403\n0.5758902\n0.5300468\n0.5107915\n0.5744110\n0.4975456\n0.2001768\n0.1311051\n0.4620202\n0\n0.0340407\n-0.0145804\n7.100000\n0.4850160\n0.0000124\n0.000000\n143.41533\n0.9372618\n22.10210\n0.2093917\n6\n0\n7\n0.0737963\n3.6830916\n4.986864\n1315.134\n9935.438\n58\n98\n183\n0.0048880\n0.7978453\n0.4455501\n1.981526\n63.57096\n6\n\n\n4_26-In-014\nd1974_1978\n2573374\n1203710\nvalidation\nwell\n0\n0\n0\n6.600000\n6.754607\n7.200000\n7.151129\n1.569213\n1.542792\n4.856076\n6.964162\n1.562499\n1.562670\n4.000438\n42.167496\n0.0000000\n0.0008454\n0.0021042\n0.3710849\n-0.9318936\n-0.0371234\n-0.0289909\n0.0029348\n-0.1056513\n0.0127788\n1.5150748\n0.2413423\n0.0020990\n-0.0706228\n-0.0113604\n-0.0272214\n-0.0301961\n-0.0346193\n-0.0273140\n-0.0343277\n0.8245047\n0.1029889\n-0.0041158\n0.0257630\n0.0017695\n-0.0331309\n0.0710320\n-0.0400928\n0.0529446\n0.8635767\n0.1616543\n-0.0062147\n0.4988544\n0.4217250\n-0.8485889\n-0.8836724\n-0.8657616\n-0.8993938\n-0.4677161\n-0.5735765\n-0.4998477\n-0.4121092\n-0.4782534\n0.3859352\n0.2732429\n0.1554769\n0.8482135\n0.8873205\n0.8635756\n0.9015982\n0.8518201\n0.5767300\n0.2149791\n0.3928713\n0.8432562\n0\n0.0686932\n-0.0085602\n8.303085\n0.3951114\n0.0000857\n0.000100\n165.80418\n0.7653937\n-20.11569\n-7.7729993\n6\n0\n7\n0.0859686\n0.0075817\n5.285522\n1315.160\n9939.923\n58\n98\n183\n0.0064054\n0.4829135\n0.4483251\n2.113142\n64.60535\n6\n\n\n4_26-In-015\nd1968_1974_ptf\n2573553\n1203935\nvalidation\nmoderate\n0\n0\n1\n6.272715\n6.272715\n6.718392\n7.269008\n1.570359\n1.541979\n4.130917\n6.945287\n1.550528\n1.562685\n4.000948\n5.479310\n0.0054557\n0.0043268\n0.0045225\n0.3907509\n4.2692256\n0.0378648\n0.6409346\n0.0022611\n-0.1020419\n0.0161510\n3.6032522\n1.8169731\n0.0346340\n0.0476020\n0.0378154\n0.2968794\n-0.0179657\n-0.0137853\n-0.0146946\n0.0060875\n1.4667766\n0.9816071\n0.0337645\n-0.0000494\n-0.3440553\n-0.0202268\n0.0882566\n-0.0308456\n0.0929077\n2.6904552\n1.0218329\n-0.0008695\n0.6999696\n0.3944107\n-0.8918364\n-0.7795515\n-0.8864348\n-0.4249992\n0.5919228\n0.4304937\n0.4614536\n0.6559467\n0.4574654\n0.4330348\n0.3299487\n0.1889674\n1.2301254\n1.8937486\n1.2098556\n1.5986075\n1.2745584\n2.7759163\n0.5375320\n0.3582314\n1.1426100\n0\n0.3005829\n0.0061576\n10.110727\n0.5134069\n0.0002062\n0.000200\n61.39244\n1.0676192\n-55.12566\n-14.0670462\n6\n0\n7\n0.0650000\n0.0007469\n5.894688\n1315.056\n9942.032\n58\n98\n183\n0.0042235\n0.6290755\n0.3974232\n2.080674\n61.16533\n6\n\n\n4_26-In-016\nd1968_1974_ptf\n2573310\n1204328\ncalibration\npoor\n0\n0\n1\n6.272715\n6.160700\n5.559031\n5.161655\n1.569434\n1.541606\n2.030315\n6.990967\n1.563066\n1.552568\n4.000725\n13.499996\n0.0000000\n0.0001476\n0.0003817\n0.1931891\n-0.1732794\n-0.1602274\n0.0318570\n-0.0035833\n-0.1282881\n0.0003549\n1.5897882\n0.8171870\n-0.0123340\n0.0400775\n-0.0813964\n0.0100844\n-0.0049875\n0.0320331\n-0.0049053\n0.0374298\n0.7912259\n0.3455668\n-0.0059622\n0.0788309\n-0.0217726\n-0.0014042\n0.1603212\n-0.0052602\n0.0867119\n1.0207798\n0.6147888\n0.0063718\n0.3157751\n0.5292308\n-0.8766075\n0.8129975\n0.5905659\n0.1640853\n0.5820994\n0.6325440\n0.8054439\n0.7448481\n0.6081498\n0.3688371\n0.2607146\n0.1763995\n1.0906221\n1.0418727\n0.8515157\n1.2106605\n0.8916541\n1.2163279\n0.4894866\n0.2049688\n0.7156029\n0\n-0.0910767\n0.0034276\n9.574804\n0.3864355\n0.0001151\n0.000525\n310.05014\n0.1321367\n-17.16055\n-28.0693741\n6\n0\n7\n0.0731646\n0.0128017\n5.938320\n1315.000\n9940.597\n58\n98\n183\n0.0040683\n0.6997021\n0.4278295\n2.041467\n55.78354\n6"
  },
  {
    "objectID": "03-model_training.html#preparations",
    "href": "03-model_training.html#preparations",
    "title": "3  Train a Random Forest",
    "section": "3.2 Preparations",
    "text": "3.2 Preparations\nBefore we can fit the model, we have to specify a few settings. First, we have to specify our target and predictor variables. Then, we have to split our dataset into a training and a testing set. Random Forest models cannot deal with NA values, so we have to remove these from our training set.\nIn the dataset we work with here, the data splitting into training and testing sets is defined by the column dataset. Usually, this is not the case and we split the data ourselves. Have a look at the section in AGDS Book on data splitting.\nAs predictors_all we use all the variables for which we have data with spatial coverage - the basis for spatial upscaling. We extracted these data in Chapter 2 from geospatial files and the data frame was constructed by cbind(), where columns number 14-104 contain the covariates data, extracted from the geospatial files. See Chapter 6 for a description of the variables. We use all of them here as predictors_all for the model.\n\n\nCode\n# Specify target: The pH in the top 10cm\ntarget &lt;- \"ph.0.10\"\n\n# Specify predictors_all: Remove soil sampling and observational data\npredictors_all &lt;- names(df_full)[14:ncol(df_full)]\n\ncat(\"The target is:\", target,\n    \"\\nThe predictors_all are:\", paste0(predictors_all[1:8], sep = \", \"), \"...\")\n\n\nThe target is: ph.0.10 \nThe predictors_all are: NegO,  PosO,  Se_MRRTF2m,  Se_MRVBF2m,  Se_NO2m_r500,  Se_PO2m_r500,  Se_SAR2m,  Se_SCA2m,  ...\n\n\n\n\nCode\n# Split dataset into training and testing sets\ndf_train &lt;- df_full |&gt; dplyr::filter(dataset == \"calibration\")\ndf_test  &lt;- df_full |&gt; dplyr::filter(dataset == \"validation\")\n\n# Filter out any NA to avoid error when running a Random Forest\ndf_train &lt;- df_train |&gt; tidyr::drop_na()\ndf_test &lt;- df_test   |&gt; tidyr::drop_na()\n\n# A little bit of verbose output:\nn_tot &lt;- nrow(df_train) + nrow(df_test)\n\nperc_cal &lt;- (nrow(df_train) / n_tot) |&gt; round(2) * 100\nperc_val &lt;- (nrow(df_test)  / n_tot) |&gt; round(2) * 100\n\ncat(\"For model training, we have a calibration / validation split of: \",\n    perc_cal, \"/\", perc_val, \"%\")\n\n\nFor model training, we have a calibration / validation split of:  75 / 25 %\n\n\nAlright, this looks all good. We have our target and predictor variables saved for easy access later on and the 75/25% split of calibration and validation data looks good, too. We can now move on to model fitting."
  },
  {
    "objectID": "03-model_training.html#model-training",
    "href": "03-model_training.html#model-training",
    "title": "3  Train a Random Forest",
    "section": "3.3 Model training",
    "text": "3.3 Model training\nThe modelling task is to predict the soil pH in the top 10 cm. Let’s start using the default hyperparameters used by ranger::ranger().\n\n\n\n\n\n\nTip\n\n\n\nHave a look at the values of the defaults by entering. ?ranger::ranger in your console and study the function documentation.\n\n\n\n\nCode\n# ranger() crashes when using tibbles, so we are using the\n# base R notation to enter the data\nrf_basic &lt;- ranger::ranger( \n  y = df_train[, target],     # target variable\n  x = df_train[, predictors_all], # Predictor variables\n  seed = 42,                    # Specify the seed for randomization to reproduce the same model again\n  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training\n\n# Print a summary of fitted model\nprint(rf_basic)\n\n\nRanger result\n\nCall:\n ranger::ranger(y = df_train[, target], x = df_train[, predictors_all],      seed = 42, num.threads = parallel::detectCores() - 1) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      605 \nNumber of independent variables:  91 \nMtry:                             9 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.3164693 \nR squared (OOB):                  0.4643405 \n\n\n\n\n\n\n\n\nPredicting categories with Random Forests\n\n\n\nIf our target variable was a categorical and not a continuous variable, we would have to set the argument probability = TRUE. The output would then be a probability map from 0-100%.\n\n\nAlthough we only used the pre-defined parameters, we already get a fairly good out-of-bag (OOB) \\(R^2\\) of 0.45 and a MSE of 0.32 pH units. See here for more background on OOB error estimation with Random Forests.\nThis is the step at which you may want to reduce the number of predictors_all to avoid collinearity and the risk of overfitting. You may also want to optimize the hyperparameters for improving the model performance and generalisability. Different hyperparameter specifications of the Random Forest model that control the model complexity may be compared. A simple way to do that is to use the {caret} R package which provides machine learning wrapper functions for hyperparameter tuning (among many more functionalities). Its use in combination with Random Forest is demonstrated in Chapter 11 of AGDS book. Reducing the number of predictors_all and retaining only the most important ones is important for obtaining robust model generalisability and is approached by what is shown below."
  },
  {
    "objectID": "03-model_training.html#variable-importance",
    "href": "03-model_training.html#variable-importance",
    "title": "3  Train a Random Forest",
    "section": "3.4 Variable importance",
    "text": "3.4 Variable importance\nOur model has 91 variables, but we don’t know anything about their role in influencing the model predictions and how important they are for achieving good predictions. ranger::ranger() provides a built-in functionality for quantifying variable importance based on the OOB-error. This functionality can be controlled with the argument importance. When set to 'permutation', the algorithm randomly permutes values of each variable, one at a time, and measures the importance as the resulting decrease in the OOB prediction skill of each decision tree within the Random Forest and returns the average across importances of all decision trees. Note that this is a model-specific variable importance quantification method. In AGDS Book Chapter 12, you have learned about a model model-agnostic method.\nThe model object returned by the ranger() function stores the variable importance information. The code below accesses this information and sorts the predictor variables with decreasing importance. If the code runs slow, you can also use the faster impurity method (see more information here).\n\n\nCode\n# Let's run the basic model again but with recording the variable importance\nrf_basic &lt;- ranger::ranger( \n  y = df_train[, target],     # target variable\n  x = df_train[, predictors_all],   # Predictor variables\n  importance   = \"permutation\", # Pick permutation to calculate variable importance\n  seed = 42,                    # Specify seed for randomization to reproduce the same model again\n  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training\n\n# Extract the variable importance and create a long tibble\nvi_rf_basic &lt;- rf_basic$variable.importance |&gt;\n  dplyr::bind_rows() |&gt; \n  tidyr::pivot_longer(cols = dplyr::everything(), names_to = \"variable\")\n\n# Plot variable importance, ordered by decreasing value\ngg &lt;- vi_rf_basic |&gt; \n  ggplot2::ggplot(ggplot2::aes(x = reorder(variable, value), y = value)) +\n  ggplot2::geom_bar(stat = \"identity\", fill = \"grey50\", width = 0.75) + \n  ggplot2::labs(\n    y = \"Change in OOB MSE after permutation\", \n    x = \"\",\n    title = \"Variable importance based on OOB\") +\n  ggplot2::theme_classic() +\n  ggplot2::coord_flip()\n\n# Display plot\ngg\n\n\n\n\n\nWhat do we see here? The higher the value, the stronger the effect of permutation on the model performance, the more important the variable. The five most important variables are the following:\n\n\n\n\n\n\n\n\nImportance rank\nVariable name\nDescription\n\n\n\n\n1\nmt_rr_y\nMean annual precipitation\n\n\n2\nmt_tt_y\nMean annual temperature\n\n\n3\nmt_td_y\nMean annual dew point temperature\n\n\n4\nmt_gh_y\nMean annual incoming radiation\n\n\n5\nbe_gwn25_vdist\nHorizontal distance to water body at 25m resolution\n\n\n\nWe find that the mean annual precipitation is by far the most important variable in determining soil pH in our model. From a soil-forming perspective, this seems plausible (Dawson, 1977). We further find that the four most important variables all describe climate - reflecting its important role as a soil-forming factor. Most of the remaining variables are metrics of the topography. It should also be noted that many of them may be correlated since. Some of them measure the same aspect of topography, but derived from a digital elevation model given at different spatial resolution (see Chapter 6). Due to their potential correlation, dropping one of the affected variables from the model may thus not lead to a strong deterioration of the model skill as its (correlated) information is still contained in the remaining variables."
  },
  {
    "objectID": "03-model_training.html#variable-selection",
    "href": "03-model_training.html#variable-selection",
    "title": "3  Train a Random Forest",
    "section": "3.5 Variable selection",
    "text": "3.5 Variable selection\nThe large number of variables in our model and the tendency that many of them exhibit a low importance in comparison to the dominating few, and that they may be correlated calls for a variable selection. Reducing the number of predictors_all reduces the risk that remaining predictors_all are correlated. Having correlated predictors_all is a problem - as shown in the context of spatial upscaling by (Ludwig et al., 2023). Intuitively, this makes sense in view of the fact that if \\(x_i\\) and \\(x_j\\) are correlated, then, for example, \\(x_i\\) is used for modelling its true association with the target variable, while \\(x_j\\) can be “spent” to model randomly occurring covariations with the target - potentially modelling noise in the data. If this happens, overfitting will follow (see here).\nDifferent strategies for reducing the number of predictors_all, while retaining model performance and improving model generalisability, exist. Greedy search, or stepwise regression are often used. Their approach is to sequentially add (stepwise forward) or remove (stepwise backward) predictors_all and to determine the best (complemented or reduced) set of predictors_all in terms of respective model performance at each step. The algorithm stops once the model starts to deteriorate or stops improving. However, it should be noted that these algorithms don’t assess all possible combinations of predictors_all and may thus not find the “globally” optimal model. A stepwise regression was implemented in AGDS I as a Report Exercise (see here).\n\n\n\n\n\n\nTip\n\n\n\nTo dig deeper into understanding how the model works, we could further investigate its partial dependence plots (see here).\n\n\nAn alternative approach to model selection is to consider the variable importance. predictors_all may be selected based on whether removing their association with the target variable (by permuting values of the predictor) deteriorates the model. Additionally, a decision criterion can be introduced for determining whether or not to retain the respective variable. This is implemented by the “Boruta-Algorithm” - an effective and popular approach to variable selection (Kursa & Rudnicki, 2010). Boruta is available as an R package {Boruta}, is based on Random Forests, and performs a permutation of variables for determining their importance - as described in Chapter 12 of AGDS Book for model-agnostic variable importance estimation. The algorithm finally categorizes variables into \"Rejected\", \"Tentative\", and \"Confirmed\". Let’s apply Boruta on our data.\n\n\nCode\nset.seed(42)\n\n# run the algorithm\nbor &lt;- Boruta::Boruta(\n    y = df_train[, target], \n    x = df_train[, predictors_all],\n    maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long\n    num.threads = parallel::detectCores()-1)\n\n# obtain results: a data frame with all variables, ordered by their importance\ndf_bor &lt;- Boruta::attStats(bor) |&gt; \n  tibble::rownames_to_column() |&gt; \n  dplyr::arrange(dplyr::desc(meanImp))\n\n# plot the importance result  \nggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), \n                             y = meanImp,\n                             fill = decision), \n                data = df_bor) +\n  ggplot2::geom_bar(stat = \"identity\", width = 0.75) + \n  ggplot2::scale_fill_manual(values = c(\"grey30\", \"tomato\", \"grey70\")) + \n  ggplot2::labs(\n    y = \"Variable importance\", \n    x = \"\",\n    title = \"Variable importance based on Boruta\") +\n  ggplot2::theme_classic() +\n  ggplot2::coord_flip()\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDetermine the length \\(N\\) of the vector of predictors_all deemed important by (\"Confirmed\") Boruta and compare these “important” variables with the \\(N\\) most important variables of the OOB-based variable importance estimation demonstrated above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the spatial upscaling in the context of digital soil mapping, let’s retain only the variables deemed important (\"Confirmed\") by the Boruta algorithm and retrain a final Random Forest model. The number of retained variables is 33.\n\n\nCode\n# get retained important variables\npredictors_selected &lt;- df_bor |&gt; \n  dplyr::filter(decision == \"Confirmed\") |&gt;\n  dplyr::pull(rowname)\n\nlength(predictors_selected)\n\n\n[1] 34\n\n\nCode\n# re-train Random Forest model\nrf_bor &lt;- ranger::ranger( \n  y = df_train[, target],              # target variable\n  x = df_train[, predictors_selected], # Predictor variables\n  seed = 42,                           # Specify the seed for randomization to reproduce the same model again\n  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training\n\n# quick report and performance of trained model object\nrf_bor\n\n\nRanger result\n\nCall:\n ranger::ranger(y = df_train[, target], x = df_train[, predictors_selected],      seed = 42, num.threads = parallel::detectCores() - 1) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      605 \nNumber of independent variables:  34 \nMtry:                             5 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.31329 \nR squared (OOB):                  0.4697219 \n\n\n\n\n\n\n\n\nTip\n\n\n\nCompare the skill of the models with all predictors_all and with the Boruta-informed reduced set of predictors_all. What is happening?\n\n\nSave the model object with the reduced set of predictors_all, calibration, and validation data for the subsequent Chapter.\n\n\nCode\n# Save relevant data for model testing in the next chapter.\nsaveRDS(rf_bor,                   \n        here::here(\"data/rf_for_ph0-10.rds\"))\n\nsaveRDS(df_train[, c(target, predictors_selected)],\n        here::here(\"data/cal_for_ph0-10.rds\"))\n\nsaveRDS(df_test[, c(target, predictors_selected)],\n        here::here(\"data/val_for_ph0-10.rds\"))\n\n\n\n\n\n\nDawson, G. A. (1977). Atmospheric ammonia from undisturbed land. Journal of Geophysical Research (1896-1977), 82(21), 3125–3133. https://doi.org/10.1029/JC082i021p03125\n\n\nKursa, M. B., & Rudnicki, W. R. (2010). Feature Selection with the Boruta Package. Journal of Statistical Software, 36(11), 1–13. https://doi.org/10.18637/jss.v036.i11\n\n\nLudwig, M., Moreno-Martinez, A., Hölzel, N., Pebesma, E., & Meyer, H. (2023). Assessing and improving the transferability of current global spatial prediction models. Global Ecology and Biogeography, 32(3), 356–368. https://doi.org/10.1111/geb.13635"
  },
  {
    "objectID": "04-model_test.html#load-model-and-data",
    "href": "04-model_test.html#load-model-and-data",
    "title": "4  Model Analysis",
    "section": "4.1 Load model and data",
    "text": "4.1 Load model and data\nThe trained model, calibration and validation data are loaded.\n\n\nCode\n# Load random forest model\nrf_bor   &lt;- readRDS(here::here(\"data/rf_for_pH0-10.rds\"))\ndf_train &lt;- readRDS(here::here(\"data/cal_for_ph0-10.rds\"))\ndf_test  &lt;- readRDS(here::here(\"data/val_for_ph0-10.rds\"))\n\n\nNext, we load a mask of the area over which the soil will be mapped. Our target area to predict over is defined in the file area_to_be_mapped.tif. Since we only want to predict on a given study area, the TIF file comes with a labeling of 0 for pixels that are outside the area of interest and 1 for pixels within the area of interest.\n\n\nCode\n# Load area to be predicted\nraster_mask &lt;- terra::rast(here::here(\"data-raw/geodata/study_area/area_to_be_mapped.tif\"))\n\n# Turn target raster into a dataframe, 1 px = 1 cell\ndf_mask &lt;- as.data.frame(raster_mask, xy = TRUE)\n\n# Filter only for area of interest\ndf_mask &lt;- df_mask |&gt; \n  dplyr::filter(area_to_be_mapped == 1)\n\n# Display df\nhead(df_mask) |&gt; \n  knitr::kable()\n\n\n\n\n\nx\ny\narea_to_be_mapped\n\n\n\n\n2587670\n1219750\n1\n\n\n2587690\n1219750\n1\n\n\n2587090\n1219190\n1\n\n\n2587090\n1219170\n1\n\n\n2587110\n1219170\n1\n\n\n2587070\n1219150\n1\n\n\n\n\n\nNext, we have to load the selected set of covariates as maps. These will be as the basis for spatial upscaling and provide the predictor values across space, fed into the trained model for predicting soil pH across space.\nGet a list of all available covariate file names.\n\n\nCode\nfiles_covariates &lt;- list.files(\n  path = here::here(\"data-raw/geodata/covariates/\"), \n  pattern = \".tif$\",\n  recursive = TRUE, \n  full.names = TRUE\n  )\n\n\nNote that the predictor rasters have to have the same resolution, extent, and coordinate reference system. This is the case as shown for two randomly picked examples.\n\n\nCode\nrandom_files &lt;- sample(files_covariates, 2)\nterra::rast(random_files[1])\n\n\nclass       : SpatRaster \ndimensions  : 986, 2428, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : 2568140, 2616700, 1200740, 1220460  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903+ / LV95 \nsource      : Se_curv2m_std_50c.tif \nname        : Se_curv2m_std_50c \nmin value   :            0.0000 \nmax value   :          213.5755 \n\n\nCode\nterra::rast(random_files[2])\n\n\nclass       : SpatRaster \ndimensions  : 986, 2428, 1  (nrow, ncol, nlyr)\nresolution  : 20, 20  (x, y)\nextent      : 2568140, 2616700, 1200740, 1220460  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903+ / LV95 \nsource      : Se_rough2m_rect3c.tif \nname        : Se_rough2m_rect3c \nmin value   :          0.000000 \nmax value   :          4.593187 \n\n\nLoad the rasters for the selected predictor variables into a raster object (a “stack” of multiple rasters).\n\n\nCode\n# Filter that list only for the variables used in the RF\npreds_selected &lt;- names(rf_bor$forest$covariate.levels)\nfiles_selected &lt;- files_covariates[apply(sapply(X = preds_selected, \n                                            FUN = grepl, \n                                            files_covariates), \n                                     MARGIN =  1, \n                                     FUN = any)]\n\n# Load all rasters as a stack\nraster_covariates &lt;- terra::rast(files_selected)\n\n\nConvert the raster stack into a dataframe - the preferred format for model prediction.\n\n\nCode\n# Get coordinates for which we want data\ndf_locations &lt;- df_mask |&gt; \n  dplyr::select(x, y)\n\n# Extract data from covariate raster stack for all gridcells in the raster\ndf_predict &lt;- terra::extract(\n  raster_covariates,   # The raster we want to extract from\n  df_locations,        # A matrix of x and y values to extract for\n  ID = FALSE           # To not add a default ID column to the output\n  )\n\ndf_predict &lt;- cbind(df_locations, df_predict) |&gt; \n  tidyr::drop_na()  # Se_TWI2m has a small number of missing data"
  },
  {
    "objectID": "04-model_test.html#model-testing",
    "href": "04-model_test.html#model-testing",
    "title": "4  Model Analysis",
    "section": "4.2 Model testing",
    "text": "4.2 Model testing\n\n4.2.1 Make predictions\nTo test our model for how well it predicts on data it has not used during model training, we first have to load the {ranger} package to load all functionalities to run a Random Forest with the predict() function. Alongside our model, we feed our validation data into the function and set its parallelization settings to use all but one of our computer’s cores.\n\n\nCode\n# Need to load {ranger} because ranger-object is used in predict()\nlibrary(ranger) \n\n# Make predictions for validation sites\nprediction &lt;- predict(\n  rf_bor,           # RF model\n  data = df_test,   # Predictor data\n  num.threads = parallel::detectCores() - 1\n  )\n\n# Save predictions to validation df\ndf_test$pred &lt;- prediction$predictions\n\n\n\n\n4.2.2 Model metrics\nNow that we have our predictions ready, we can extract standard metrics for a classification problem (see AGDS Chapter 8.2.2).\n\n\nCode\n# Calculate error\nerr &lt;- df_test$ph.0.10 - df_test$pred\n\n# Calculate bias\nbias &lt;- mean(err, na.rm = TRUE) |&gt; round(2)\n\n# Calculate RMSE\nrmse &lt;- sqrt(mean(err, na.rm = TRUE)) |&gt; round(2)\n\n# Calculate R2\nr2 &lt;- cor(df_test$ph.0.10, df_test$pred, method = \"pearson\")^2 |&gt; round(2)\n\n\n\n\n4.2.3 Metric plots\n\n\nCode\ndf_test |&gt; \n  ggplot2::ggplot(ggplot2::aes(x = pred, y = ph.0.10)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_smooth(method = \"lm\",\n                       color = \"tomato\") +\n  ggplot2::theme_classic() +\n  ggplot2::geom_abline(\n    intercept = 0, \n    slope = 1, \n    linetype = \"dotted\") +\n  ggplot2::ylim(5, 7.5) +\n  ggplot2::xlim(5, 7.5) +\n  ggplot2::labs(\n    title = \"Predicted vs. Observed soil pH 0-10 cm\",\n    subtitle = bquote(paste(\"Bias = \", .(bias), \n                            \", RMSE = \", .(rmse), \n                            \", R\"^2, \" = \", .(r2))),\n    x = \"Predicted pH\",\n    y = \"Observed pH\"\n  )\n\n\n\n\n\nFigure 4.1: Comparison of observed versus predicted values for top soil pH using a simple Random Forest model.\n\n\n\n\nThe plot shows that our model explains about half of the observed variation in soil pH. Yet, we can also see that the model tends to overestimate low pH values. Anyways, let’s move ahead."
  },
  {
    "objectID": "04-model_test.html#create-prediction-maps",
    "href": "04-model_test.html#create-prediction-maps",
    "title": "4  Model Analysis",
    "section": "4.3 Create prediction maps",
    "text": "4.3 Create prediction maps\nThe fitted and tested model can now be used for spatially upscaling - creating a map of top soil pH values across our study area. For this, we again make predictions with our Random Forest model but we use our covariates dataframe for the study area, instead of only at the sampling locations as done above.\n\n\nCode\n# Make predictions using the RF model\nprediction &lt;- predict(\n  rf_bor,              # RF model\n  data = df_predict,   \n  num.threads = parallel::detectCores() - 1)\n\n# Attach predictions to dataframe and round them\ndf_predict$prediction &lt;- prediction$predictions\n\n\n\n\nCode\n# Extract dataframe with coordinates and predictions\ndf_map &lt;- df_predict |&gt;\n  dplyr::select(x, y, prediction)\n\n# Turn dataframe into a raster\nraster_pred &lt;- terra::rast(\n  df_map,                  # Table to be transformed\n  crs = \"+init=epsg:2056\", # Swiss coordinate system\n  extent = terra::ext(raster_covariates) # Prescribe same extent as predictor rasters\n  )\n\n\n\n\nCode\n# Let's have a look at our predictions!\n# To have some more flexibility, we can plot this in the ggplot-style as such:\nggplot2::ggplot() +\n  tidyterra::geom_spatraster(data = raster_pred) +\n  ggplot2::scale_fill_viridis_c(\n    na.value = NA,\n    option = \"viridis\",\n    name = \"pH\"\n    ) +\n  ggplot2::theme_classic() +\n  ggplot2::scale_x_continuous(expand = c(0, 0)) +\n  ggplot2::scale_y_continuous(expand = c(0, 0)) +\n  ggplot2::labs(title = \"Predicted soil pH (0 - 10cm)\")\n\n\n\n\n\nFigure 4.2: Predicted map of top soil pH using a simple Random Forest model.\n\n\n\n\nInteresting, we see that in this study area, there is a tendency of having more acidic soils towards the south west and more basic soils towards the north east.\nLet’s write the predicted top soil pH raster into a GeoTIFF file.\n\n\nCode\n# Save raster as .tif file\nterra::writeRaster(\n  raster_pred,\n  \"../data/ra_predicted_ph0-10.tif\",\n  datatype = \"FLT4S\",  # FLT4S for floats, INT1U for integers (smaller file)\n  filetype = \"GTiff\",  # GeoTiff format\n  overwrite = TRUE     # Overwrite existing file\n)\n\n\nThat’s it."
  },
  {
    "objectID": "99-exercise.html#simple-model",
    "href": "99-exercise.html#simple-model",
    "title": "5  Report Exercise",
    "section": "5.1 Simple model",
    "text": "5.1 Simple model\nRe-implement the digital soil mapping workflow, using Random Forest, as demonstrated in this tutorial, but for the binary categorical variable waterlog.100. Here are a few hints as a guide:\n\nMake sure that the categorical target variable is encoded as a factor using the function factor().\nStart with a model that includes all predictors, trained on the pre-defined training subset.\nEvaluate the model on the testing subset of the data. Consider appropriate metrics as described in AGDS Book Chapter 8.3. Is the data balanced in terms of observed TRUE and FALSE values? What does this imply for the interpretation of the different metrics?"
  },
  {
    "objectID": "99-exercise.html#variable-selection",
    "href": "99-exercise.html#variable-selection",
    "title": "5  Report Exercise",
    "section": "5.2 Variable selection",
    "text": "5.2 Variable selection\n\nReduce the predictor set as demonstrated in this tutorial.\nRepeat the model evaluation and compare the model performance on the test set with what was obtained with the model using all available covariates. Which model generalises better to unseen data?\nWould the same model choice be made if we considered the OOB prediction error reported as part of the trained model object?"
  },
  {
    "objectID": "99-exercise.html#model-optimization",
    "href": "99-exercise.html#model-optimization",
    "title": "5  Report Exercise",
    "section": "5.3 Model optimization",
    "text": "5.3 Model optimization\nIn AGDS Book Chapter 11, you learned how to optimize hyperparameters using cross-validation. Using the training data subset, implement a 5-fold cross-validation to optimise the hyperparameters mtry and min.node.size of the same Random Forest model as implemented above. You may use the {caret} library as demonstrated in AGDS Book. Evaluate the optimized model on the test set using the same metrics as considered above. Does the model generalise better to unseen data than the initial model (which used default hyperparameters, see ?ranger::ranger)."
  },
  {
    "objectID": "99-exercise.html#probabilistic-predictions",
    "href": "99-exercise.html#probabilistic-predictions",
    "title": "5  Report Exercise",
    "section": "5.4 Probabilistic predictions",
    "text": "5.4 Probabilistic predictions\nUsing the optimised (or if you didn’t manage - the initial default) hyperparameters, train the Random Forest model, setting ranger::ranger(..., probability = TRUE). This yields not a model predicting a binary class, but a probability of the target to be TRUE. This lets the user chose where to put the threshold for translating a probability to a binary class. E.g., if the predicted probability is \\(&gt;0.5\\), then consider this as a prediction of TRUE. Establish the Reicever-operating-characteristic curve, as described in AGDS Book Chapter 8.3.\nConsider you inform an infrastructure construction project where waterlogged soils severely jeopardize the stability of the building. Then, consider you inform a project where waterlogged soils are unwanted, but not critical. In both cases, your prediction map of a binary classification is used as a basis and the binary classification is derived from the probabilistic prediction. How would you chose the threshold in each case? Would you chose the same threshold in both cases? If not, explain why. Can you think of an analogy of a similarly-natured problem in another realm?"
  },
  {
    "objectID": "99-variables.html",
    "href": "99-variables.html",
    "title": "6  Variables",
    "section": "",
    "text": "Detailed information can be found in folder resources on GitHub. Information on the individual metrics can be found online.\n\n\n\n\n\n\n\n\n\nFilename\nMeaning\nUnit\n\n\n\n\nbe_gwn25_hdist.tif\nHorizontal distance to water body at 25m resolution\nm\n\n\nbe_gwn25_vdist.tif\nHorizontal distance to water body at 25m resolution\nm\n\n\ncindx10_25.tif\nConvergence index at 25m resolution, taken over a radius of 10 pixels\nindex\n\n\ncindx50_25.tif\nConvergence index at 25m resolution, taken over a radius of 50 pixels\nindex\n\n\ngeo500h1id.tif\nGeological map 1:500’000 for ground water presence\ncategorical\n\n\ngeo500h3id.tif\nGeological map 1:500’000 for ground layer\ncategorical\n\n\nlgm.tif\nLast glacial maximum\ncategorical, ages ago\n\n\nlsf.tif\nSlope length and steepness factor\nindex\n\n\nmrrtf25.tif\nMulti-resolution ridge top flatness at 25m resolution\nindex\n\n\nmrvbf25.tif\nMulti-resolution valley bottom flatness at 25m resolution\nindex\n\n\nmt_gh_y.tif\nMean annual incoming radiation\n0.1 W/m2\n\n\nmt_rr_y.tif\nMean annual precipitation\nmm\n\n\nmt_td_y.tif\nMean annual dew point temperature\n0.1 ºC\n\n\nmt_tt_y.tif\nMean annual temperature\n0.1 ºC\n\n\nmt_ttvar.tif\nTemperature variation (= mean temperature in January - mean temperature in Juli)\n0.1 ºC\n\n\nNegO.tif\nTopographic negative openness\nindex\n\n\nPosO.tif\nTopographic positive openness\nindex\n\n\nprotindx.tif\nMorphometric protection index at 25m resolution\nindex\n\n\nSe_alti2m_std_50c.tif\nStandard deviation of elevation at 50 pixels resolution\nm\n\n\nSe_conv2m.tif\nConvergence index at 2 m resolution\nº\n\n\nSe_curv25m.tif\nCurvature at 25m resolution\n1/m\n\n\nSe_curv2m_fmean_50c.tif\nCurvature at 2 m resolution, mean over 50 pixels\n1/m\n\n\nSe_curv2m_fmean_5c.tif\nCurvature at 2 m resolution, mean over 5 pixels\n1/m\n\n\nSe_curv2m_s60.tif\nCurvature at 2 m resolution, smoothed over area with radius of 60 pixels\n1/m\n\n\nSe_curv2m_std_50c.tif\nCurvature at 2 m resolution, standard deviation over 50 pixels\n1/m\n\n\nSe_curv2m_std_5c.tif\nCurvature at 2 m resolution, standard deviation over 5 pixels\n1/m\n\n\nSe_curv2m.tif\nCurvature at 2 m resolution\n1/m\n\n\nSe_curv50m.tif\nCurvature at 50 m resolution\n1/m\n\n\nSe_curv6m.tif\nCurvature at 6 m resolution\n1/m\n\n\nSe_curvplan25m.tif\nCurvature plan (planform curvature) at 25m resolution\n1/m\n\n\nSe_curvplan2m_fmean_50c.tif\nCurvature plan (planform curvature) at 2 m resolution, mean over 50 pixels\n1/m\n\n\nSe_curvplan2m_fmean_5c.tif\nCurvature plan (planform curvature) at 2 m resolution, mean over 5 pixels\n1/m\n\n\nSe_curvplan2m_s60.tif\nCurvature plan (planform curvature) at 2 m resolution, smoothed over area with radius of 60 pixels\n1/m\n\n\nSe_curvplan2m_s7.tif\nCurvature plan (planform curvature) at 2 m resolution, smoothed over area with radius of 7 pixels\n1/m\n\n\nSe_curvplan2m_std_50c.tif\nCurvature plan (planform curvature) at 2 m resolution, standard deviation over 50 pixels\n1/m\n\n\nSe_curvplan2m_std_5c.tif\nCurvature plan (planform curvature) at 2 m resolution, standard deviation over 5 pixels\n1/m\n\n\nSe_curvplan2m.tif\nCurvature plan (planform curvature) at 2 m resolution\n1/m\n\n\nSe_curvplan50m.tif\nCurvature plan (planform curvature) at 50 m resolution\n1/m\n\n\nSe_curvprof25m.tif\nCurvature profile (profile curvature) at 25m resolution\n1/m\n\n\nSe_curvprof2m_fmean_50c.tif\nCurvature profile (profile curvature) at 2 m resolution, mean over 50 pixels\n1/m\n\n\nSe_curvprof2m_fmean_5c.tif\nCurvature profile (profile curvature) at 2 m resolution, mean over 5 pixels\n1/m\n\n\nSe_curvprof2m_s60.tif\nCurvature profile (profile curvature) at 2 m resolution, smoothed over area with radius of 60 pixels\n1/m\n\n\nSe_curvprof2m_s7.tif\nCurvature profile (profile curvature) at 2 m resolution, smoothed over area with radius of 7 pixels\n1/m\n\n\nSe_curvprof2m_std_50c.tif\nCurvature profile (profile curvature) at 2 m resolution, standard deviation over 50 pixels\n1/m\n\n\nSe_curvprof2m_std_5c.tif\nCurvature profile (profile curvature) at 2 m resolution, standard deviation over 5 pixels\n1/m\n\n\nSe_curvprof2m.tif\nCurvature profile (profile curvature) at 2 m resolution\n1/m\n\n\nSe_curvprof50m.tif\nCurvature profile (profile curvature) at 50 m resolution\n1/m\n\n\nSe_diss2m_50c.tif\nDissection at 2 m resolution over area with radius of 50 pixels\nm\n\n\nSe_diss2m_5c.tif\nDissection at 2 m resolution over area with radius of 5 pixels\nm\n\n\nSe_e_aspect25m.tif\nAspect - Eastness at 25m resolution (E=90º, cos(Aspect in Rad.))\nº\n\n\nSe_e_aspect2m_5c.tif\nAspect - Eastness at 2 m resolution, mean over area with radius of 5 pixels\nº\n\n\nSe_e_aspect2m.tif\nAspect - Eastness at 2 m resolution\nº\n\n\nSe_e_aspect50m.tif\nAspect - Eastness at 50 m resolution\nº\n\n\nSe_MRRTF2m.tif\nMulti-resolution ridge top flatness at 2 m resolution\nindex\n\n\nSe_MRVBF2m.tif\nMulti-resolution valley bottom flatness at 2 m resolution\nindex\n\n\nSe_n_aspect2m_50c.tif\nAspect - Northness at 2 m resolution, mean over area with radius of 50 pixels\nº\n\n\nSe_n_aspect2m_5c.tif\nAspect - Northness at 2 m resolution, mean over area with radius of 5 pixels\nº\n\n\nSe_n_aspect2m.tif\nAspect - Northness at 2 m resolution (N=90º, sin(Aspect in Rad.))\nº\n\n\nSe_n_aspect50m.tif\nAspect - Northness at 50 m resolution\nº\n\n\nSe_n_aspect6m.tif\nAspect - Northness at 6 m resolution\nº\n\n\nSe_NO2m_r500.tif\nTopographic negative openness at 2 m resolution, over area with radius of 500 pixels\nindex\n\n\nSe_PO2m_r500.tif\nTopographic positive openness at 2 m resolution, over area with radius of 500 pixels\nindex\n\n\nSe_rough2m_10c.tif\nRoughness at 2 m resolution, over area of with radius of 10 pixels\nm\n\n\nSe_rough2m_5c.tif\nRoughness at 2 m resolution, over area of with radius of 5 pixels\nm\n\n\nSe_rough2m_rect3c.tif\nRoughness at 2 m resolution, over rectangular area of 3 pixels\nm\n\n\nSe_SAR2m.tif\nSurface area ratio at 2 m resolution\nratio\n\n\nSe_SCA2m.tif\nSpecific catchment area (contributing area, multi-flow) at 2 m resolution\nsqm\n\n\nSe_slope2m_fmean_50c.tif\nSlope at 2 m resolution, smoothed mean over 50 pixels\nº\n\n\nSe_slope2m_fmean_5c.tif\nSlope at 2 m resolution, smoothed mean over 5 pixels\nº\n\n\nSe_slope2m_s60.tif\nSlope at 2 m resolution, weighted smoothing over area with radius of 60 pixels\nº\n\n\nSe_slope2m_s7.tif\nSlope at 2 m resolution, weighted smoothing over area with radius of 7 pixels\nº\n\n\nSe_slope2m_std_50c.tif\nSlope at 2 m resolution, standard deviation over 50 pixels\nº\n\n\nSe_slope2m_std_5c.tif\nSlope at 2 m resolution, standard deviation over 5 pixels\nº\n\n\nSe_slope2m.tif\nSlope at 2 m resolution\nº\n\n\nSe_slope50m.tif\nSlope at 50 m resolution\nº\n\n\nSe_slope6m.tif\nSlope at 6 m resolution\nº\n\n\nSe_toposcale2m_r3_r50_i10s.tif\nToposcale at 2 m resolution with intervals of 10 pixels\nindex\n\n\nSe_tpi_2m_50c.tif\nTopographic position index at 2 m resolution, taken over area with radius of 50 pixels\nindex\n\n\nSe_tpi_2m_5c.tif\nTopographic position index at 2 m resolution, taken over area with radius of 5 pixels\nindex\n\n\nSe_tri2m_altern_3c.tif\nTerrain ruggedness index at 2 m resolution, taken over rectangular area of 3x3 pixels\nindex\n\n\nSe_tsc10_2m.tif\nTerrain surface convexity at 2 m resolution, taken over radius of 15 pixels\nindex\n\n\nSe_TWI2m_s15.tif\nTopographic wetness index at 2 m resolution, taken over radius of 15 pixels\nindex\n\n\nSe_TWI2m_s60.tif\nTopographic wetness index at 2 m resolution, taken over radius of 60 pixels\nindex\n\n\nSe_TWI2m.tif\nTopographic wetness index at 2 m resolution\nindex\n\n\nSe_vrm2m_r10c.tif\nVector ruggedness measure at 2 m resolution, taken over radius of 10 pixels\nnone\n\n\nSe_vrm2m.tif\nVector ruggedness measure at 2 m resolution\nnone\n\n\nterrTextur.tif\nTerrain texture at 2 m resolution\nna\n\n\ntsc25_18.tif\nSurface convexity at 25m resolution\nindex\n\n\ntsc25_40.tif\nSurface convexity at 25m resolution\nindex\n\n\nvdcn25.tif\nVertical distance to drainage to channel networks at 25m resolution\nm\n\n\nvszone.tif\nDrainage zones at 25m resolution\ncategorical"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dawson, G. A. (1977). Atmospheric ammonia from undisturbed land.\nJournal of Geophysical Research (1896-1977), 82(21),\n3125–3133. https://doi.org/10.1029/JC082i021p03125\n\n\nHengl, T., Nussbaum, M., Wright, M. N., Heuvelink, G. B. M., &\nGräler, B. (2018). Random forest as a generic framework for predictive\nmodeling of spatial and spatio-temporal variables. PeerJ,\n6, e5518. https://doi.org/10.7717/peerj.5518\n\n\nJenny, H. (1994). Factors of soil formation: A system of\nquantitative pedology. Dover.\n\n\nKursa, M. B., & Rudnicki, W. R. (2010). Feature\nSelection with the Boruta\nPackage. Journal of Statistical Software,\n36(11), 1–13. https://doi.org/10.18637/jss.v036.i11\n\n\nLudwig, M., Moreno-Martinez, A., Hölzel, N., Pebesma, E., & Meyer,\nH. (2023). Assessing and improving the transferability of current global\nspatial prediction models. Global Ecology and Biogeography,\n32(3), 356–368. https://doi.org/10.1111/geb.13635\n\n\nNussbaum, M., Spiess, K., Baltensweiler, A., Grob, U., Keller, A.,\nGreiner, L., Schaepman, M. E., & Papritz, A. (2018). Evaluation of\ndigital soil mapping approaches with large sets of environmental\ncovariates. SOIL, 4(1), 1–22. https://doi.org/10.5194/soil-4-1-2018\n\n\nNussbaum, M., Walthert, L., Fraefel, M., Greiner, L., & Papritz, A.\n(2017). Mapping of soil properties at high resolution in Switzerland\nusing boosted geoadditive models. SOIL, 3(4), 191–210.\nhttps://doi.org/10.5194/soil-3-191-2017"
  }
]