{
  "hash": "f61ebad2aad76ad58d5fd86a34ce455b",
  "result": {
    "markdown": "# Fitting a Random Forest Model {#sec-modelfit}\n\n## Load data\n\nIn the previous Chapter, we create a dataframe that holds information on the soil sampling locations and the covariates that we extracted for these positions. Let's load this datafarme into our environment\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_clean <- readRDS(here::here(\"data/bern_sampling_locations_with_covariates.rds\"))\n\nhead(data_clean) |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|site_id_unique |timeset        |       x|       y|dataset     |dclass   | waterlog.30| waterlog.50| waterlog.100|  ph.0.10| ph.10.30| ph.30.50| ph.50.100| be_gwn25_hdist| be_gwn25_vdist| cindx10_25|  cindx50_25|geo500h1id |geo500h3id |lgm |       lsf|   mrrtf25|  mrvbf25|  mt_gh_y|  mt_rr_y| mt_td_y| mt_tt_y| mt_ttvar|     NegO|     PosO|  protindx| Se_alti2m_std_50c|   Se_conv2m| Se_curv25m| Se_curv2m_fmean_50c| Se_curv2m_fmean_5c| Se_curv2m_s60| Se_curv2m_std_50c| Se_curv2m_std_5c|  Se_curv2m| Se_curv50m|  Se_curv6m| Se_curvplan25m| Se_curvplan2m_fmean_50c| Se_curvplan2m_fmean_5c| Se_curvplan2m_s60| Se_curvplan2m_s7| Se_curvplan2m_std_50c| Se_curvplan2m_std_5c| Se_curvplan2m| Se_curvplan50m| Se_curvprof25m| Se_curvprof2m_fmean_50c| Se_curvprof2m_fmean_5c| Se_curvprof2m_s60| Se_curvprof2m_s7| Se_curvprof2m_std_50c| Se_curvprof2m_std_5c| Se_curvprof2m| Se_curvprof50m| Se_diss2m_50c| Se_diss2m_5c| Se_e_aspect25m| Se_e_aspect2m_5c| Se_e_aspect2m| Se_e_aspect50m| Se_MRRTF2m| Se_MRVBF2m| Se_n_aspect2m_50c| Se_n_aspect2m_5c| Se_n_aspect2m| Se_n_aspect50m| Se_n_aspect6m| Se_NO2m_r500| Se_PO2m_r500| Se_rough2m_10c| Se_rough2m_5c| Se_rough2m_rect3c| Se_SAR2m|  Se_SCA2m| Se_slope2m_fmean_50c| Se_slope2m_fmean_5c| Se_slope2m_s60| Se_slope2m_s7| Se_slope2m_std_50c| Se_slope2m_std_5c| Se_slope2m| Se_slope50m| Se_slope6m| Se_toposcale2m_r3_r50_i10s| Se_tpi_2m_50c| Se_tpi_2m_5c| Se_tri2m_altern_3c| Se_tsc10_2m| Se_TWI2m_s15| Se_TWI2m_s60|  Se_TWI2m| Se_vrm2m_r10c|  Se_vrm2m| terrTextur|  tsc25_18| tsc25_40|   vdcn25|vszone |     x30|     y30|      x60|     y60|\n|:--------------|:--------------|-------:|-------:|:-----------|:--------|-----------:|-----------:|------------:|--------:|--------:|--------:|---------:|--------------:|--------------:|----------:|-----------:|:----------|:----------|:---|---------:|---------:|--------:|--------:|--------:|-------:|-------:|--------:|--------:|--------:|---------:|-----------------:|-----------:|----------:|-------------------:|------------------:|-------------:|-----------------:|----------------:|----------:|----------:|----------:|--------------:|-----------------------:|----------------------:|-----------------:|----------------:|---------------------:|--------------------:|-------------:|--------------:|--------------:|-----------------------:|----------------------:|-----------------:|----------------:|---------------------:|--------------------:|-------------:|--------------:|-------------:|------------:|--------------:|----------------:|-------------:|--------------:|----------:|----------:|-----------------:|----------------:|-------------:|--------------:|-------------:|------------:|------------:|--------------:|-------------:|-----------------:|--------:|---------:|--------------------:|-------------------:|--------------:|-------------:|------------------:|-----------------:|----------:|-----------:|----------:|--------------------------:|-------------:|------------:|------------------:|-----------:|------------:|------------:|---------:|-------------:|---------:|----------:|---------:|--------:|--------:|:------|-------:|-------:|--------:|-------:|\n|4_26-In-005    |d1968_1974_ptf | 2571994| 1203001|validation  |poor     |           0|           0|            1| 6.071733| 6.227780| 7.109235|  7.214589|      234.39087|      1.2986320|  -10.62191|  -6.9658718|6          |0          |7   | 0.0770846| 0.0184651| 4.977099| 1316.922| 9931.120|      58|      98|      183| 1.569110| 1.534734| 0.0159717|         0.3480562| -40.5395088| -0.0014441|          -0.0062570|          0.0175912|     0.0002296|         2.9204133|        1.1769447| -1.9364884|  0.0031319| -0.5886537|     -0.0042508|              -0.0445323|             -0.0481024|        -0.0504083|       -0.1655090|             1.5687343|            0.6229440|    -1.0857303|      0.0007920|     -0.0028067|              -0.0382753|             -0.0656936|        -0.0506380|       -0.0732220|             1.6507173|            0.7082230|     0.8507581|     -0.0023399|     0.3934371|    0.1770810|     -0.9702092|       -0.7929600|    -0.5661940|     -0.9939429|   5.930607|   6.950892|        -0.2840056|       -0.6084610|    -0.2402939|     -0.0577110|    -0.7661251|     1.562085|     1.548762|      0.3228087|     0.2241062|         0.2003846| 4.000910| 16.248077|            0.9428899|           0.6683306|      0.9333237|     0.7310556|          0.8815832|         0.3113754|  1.1250136|   0.3783818|  0.5250366|                          0|    -0.0940372|   -0.0583917|          10.319408|   0.4645128|    0.0032796|    0.0049392| 0.0011592|      0.000125| 0.0002450|  0.6248673| 0.3332805| 1.784737| 65.62196|6      | 1625912| 2327826| 244167.6| 2828913|\n|4_26-In-006    |d1974_1978     | 2572149| 1202965|calibration |poor     |           0|           1|            1| 6.900000| 6.947128| 7.203502|  7.700000|      127.41681|      1.7064546|  -10.87862| -11.8201790|6          |0          |7   | 0.0860347| 0.0544361| 4.975796| 1317.000| 9931.672|      58|      98|      183| 1.568917| 1.533827| 0.0204794|         0.1484705|  19.0945148| -0.0190294|           0.0021045|          0.0221433|     0.0000390|         3.8783867|        4.3162045|  2.1377332| -0.0171786|  0.1278165|     -0.0119618|              -0.0501855|             -0.3270764|        -0.1004921|       -0.5133076|             2.0736780|            2.2502327|    -0.3522736|     -0.0073879|      0.0070676|              -0.0522900|             -0.3492197|        -0.1005311|       -0.4981292|             2.1899190|            2.4300070|    -2.4900069|      0.0097907|     0.4014700|    0.7360508|      0.5683194|        0.8753148|    -0.3505180|      0.3406741|   5.984921|   6.984581|        -0.5732749|        0.4801802|     0.4917848|     -0.4550385|     0.7722272|     1.543384|     1.558683|      0.2730940|     0.2489859|         0.2376962| 4.001326|  3.357315|            1.0895698|           0.9857153|      1.0231543|     1.0398037|          1.0152543|         0.5357812|  1.3587183|   0.0645478|  0.5793087|                          0|    -0.0014692|    0.0180000|          12.603136|   0.5536283|    0.0070509|    0.0067992| 0.0139006|      0.000300| 0.0005389|  0.7573612| 0.3395441| 1.832904| 69.16074|6      | 1626064| 2327873| 244276.3| 2829029|\n|4_26-In-012    |d1974_1978     | 2572937| 1203693|calibration |moderate |           0|           1|            1| 6.200000| 6.147128| 5.603502|  5.904355|      143.41533|      0.9372618|   22.10210|   0.2093917|6          |0          |7   | 0.0737963| 3.6830916| 4.986864| 1315.134| 9935.438|      58|      98|      183| 1.569093| 1.543057| 0.0048880|         0.1112066|  -9.1396294|  0.0039732|           0.0009509|          0.0431735|     0.0034232|         0.7022317|        0.4170935| -0.4178924| -0.0026431| -0.0183221|      0.0015183|              -0.0079620|              0.0053904|        -0.0091239|       -0.0110896|             0.3974485|            0.2292406|    -0.2168447|     -0.0013561|     -0.0024548|              -0.0089129|             -0.0377831|        -0.0125471|       -0.0052359|             0.4158890|            0.2700820|     0.2010477|      0.0012870|     0.6717541|    0.4404107|     -0.6987815|       -0.3866692|    -0.1960597|     -0.7592779|   5.953919|   6.990917|        -0.3006475|       -0.9221049|    -0.9633239|     -0.3257418|    -0.9502072|     1.565405|     1.563151|      0.2305476|     0.2182523|         0.1434273| 4.000320| 11.330072|            0.5758902|           0.5300468|      0.5107915|     0.5744110|          0.4975456|         0.2001768|  0.7160403|   0.1311051|  0.4620202|                          0|     0.0340407|   -0.0145804|           7.100000|   0.4850160|    0.0021498|    0.0017847| 0.0011398|      0.000000| 0.0000124|  0.7978453| 0.4455501| 1.981526| 63.57096|6      | 1626382| 2328897| 244039.8| 2830075|\n|4_26-In-014    |d1974_1978     | 2573374| 1203710|validation  |well     |           0|           0|            0| 6.600000| 6.754607| 7.200000|  7.151129|      165.80418|      0.7653937|  -20.11569|  -7.7729993|6          |0          |7   | 0.0859686| 0.0075817| 5.285522| 1315.160| 9939.923|      58|      98|      183| 1.569213| 1.542792| 0.0064054|         0.3710849|  -0.9318936| -0.0371234|           0.0029348|         -0.1056513|     0.0127788|         1.5150748|        0.2413423| -0.0289909|  0.0020990| -0.0706228|     -0.0113604|              -0.0301961|             -0.0346193|        -0.0273140|       -0.0343277|             0.8245047|            0.1029889|    -0.0272214|     -0.0041158|      0.0257630|              -0.0331309|              0.0710320|        -0.0400928|        0.0529446|             0.8635767|            0.1616543|     0.0017695|     -0.0062147|     0.4988544|    0.4217250|     -0.8485889|       -0.8657616|    -0.8836724|     -0.8993938|   4.856076|   6.964162|        -0.5735765|       -0.4998477|    -0.4677161|     -0.4121092|    -0.4782534|     1.562499|     1.562670|      0.3859352|     0.2732429|         0.1554769| 4.000438| 42.167496|            0.8873205|           0.8635756|      0.9015982|     0.8518201|          0.5767300|         0.2149791|  0.8482135|   0.3928713|  0.8432562|                          0|     0.0686932|   -0.0085602|           8.303085|   0.3951114|    0.0008454|    0.0021042| 0.0000000|      0.000100| 0.0000857|  0.4829135| 0.4483251| 2.113142| 64.60535|6      | 1626752| 2329130| 244243.6| 2830462|\n|4_26-In-015    |d1968_1974_ptf | 2573553| 1203935|validation  |moderate |           0|           0|            1| 6.272715| 6.272715| 6.718392|  7.269008|       61.39244|      1.0676192|  -55.12566| -14.0670462|6          |0          |7   | 0.0650000| 0.0007469| 5.894688| 1315.056| 9942.032|      58|      98|      183| 1.570359| 1.541979| 0.0042235|         0.3907509|   4.2692256|  0.0378648|           0.0022611|         -0.1020419|     0.0161510|         3.6032522|        1.8169731|  0.6409346|  0.0346340|  0.0476020|      0.0378154|              -0.0179657|             -0.0137853|        -0.0146946|        0.0060875|             1.4667766|            0.9816071|     0.2968794|      0.0337645|     -0.0000494|              -0.0202268|              0.0882566|        -0.0308456|        0.0929077|             2.6904552|            1.0218329|    -0.3440553|     -0.0008695|     0.6999696|    0.3944107|     -0.8918364|       -0.8864348|    -0.7795515|     -0.4249992|   4.130917|   6.945287|         0.4304937|        0.4614536|     0.5919228|      0.6559467|     0.4574654|     1.550528|     1.562685|      0.4330348|     0.3299487|         0.1889674| 4.000948|  5.479310|            1.8937486|           1.2098556|      1.5986075|     1.2745584|          2.7759163|         0.5375320|  1.2301254|   0.3582314|  1.1426100|                          0|     0.3005829|    0.0061576|          10.110727|   0.5134069|    0.0043268|    0.0045225| 0.0054557|      0.000200| 0.0002062|  0.6290755| 0.3974232| 2.080674| 61.16533|6      | 1626795| 2329415| 244138.2| 2830730|\n|4_26-In-016    |d1968_1974_ptf | 2573310| 1204328|calibration |poor     |           0|           0|            1| 6.272715| 6.160700| 5.559031|  5.161655|      310.05014|      0.1321367|  -17.16055| -28.0693741|6          |0          |7   | 0.0731646| 0.0128017| 5.938320| 1315.000| 9940.597|      58|      98|      183| 1.569434| 1.541606| 0.0040683|         0.1931891|  -0.1732794| -0.1602274|          -0.0035833|         -0.1282881|     0.0003549|         1.5897882|        0.8171870|  0.0318570| -0.0123340|  0.0400775|     -0.0813964|              -0.0049875|              0.0320331|        -0.0049053|        0.0374298|             0.7912259|            0.3455668|     0.0100844|     -0.0059622|      0.0788309|              -0.0014042|              0.1603212|        -0.0052602|        0.0867119|             1.0207798|            0.6147888|    -0.0217726|      0.0063718|     0.3157751|    0.5292308|     -0.8766075|        0.5905659|     0.8129975|      0.1640853|   2.030315|   6.990967|         0.6325440|        0.8054439|     0.5820994|      0.7448481|     0.6081498|     1.563066|     1.552568|      0.3688371|     0.2607146|         0.1763995| 4.000725| 13.499996|            1.0418727|           0.8515157|      1.2106605|     0.8916541|          1.2163279|         0.4894866|  1.0906221|   0.2049688|  0.7156029|                          0|    -0.0910767|    0.0034276|           9.574804|   0.3864355|    0.0001476|    0.0003817| 0.0000000|      0.000525| 0.0001151|  0.6997021| 0.4278295| 2.041467| 55.78354|6      | 1626388| 2329634| 243676.4| 2830716|\n:::\n:::\n\n\n## Preparations\n\nBefore we can fit the model, we have to specify a few settings. First, we have to specify our response and predictor variables. Then, we have to split our dataset into a calibration and a validation set. Random Forest models cannot deal with `NA` values, so we have to remove these from our calibration set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify response: The pH in the top 10cm\nresponse <- \"ph.0.10\"\n\n# Specify predictors: Remove soil sampling information\npredictors <- \n  data_clean |> \n  dplyr::select(-response,                             # Remove response variable\n                -site_id_unique,                       # Remove site ID\n                -tidyr::starts_with(\"ph\"),             # No pH information\n                -tidyr::starts_with(\"waterlog\"),       # No water-status information\n                -dclass,                               # No water-status information\n                -dataset) |>                           # No calib./valid information\n  names()\n\ncat(\"The response is:\", response,\n    \"\\nThe predictors are:\", paste0(predictors[1:8], sep = \", \"), \"...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe response is: ph.0.10 \nThe predictors are: timeset,  x,  y,  be_gwn25_hdist,  be_gwn25_vdist,  cindx10_25,  cindx50_25,  geo500h1id,  ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split dataset into calibration and validation\ndata_cal <- data_clean |> dplyr::filter(dataset == \"calibration\")\ndata_val <- data_clean |> dplyr::filter(dataset == \"validation\")\n\n# Filter out any NA to avoid error when running a Random Forest\ndata_cal <- data_cal |> tidyr::drop_na()\ndata_val <- data_val |> tidyr::drop_na()\n\n# A little bit of verbose output:\nn_tot <- nrow(data_cal) + nrow(data_val)\n\nperc_cal <- (nrow(data_cal) / n_tot) |> round(2) * 100\nperc_val <- (nrow(data_val) / n_tot) |> round(2) * 100\n\ncat(\"For model training, we have a calibration / validation split of: \",\n    perc_cal, \"/\", perc_val, \"%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor model training, we have a calibration / validation split of:  75 / 25 %\n```\n:::\n:::\n\n\nAlright, this looks all good. We have our response and predictor variables saved for easy access later on and the 75/25 split of calibration and validation data looks good too. We can now move on to model fitting!\n\n## Model fitting\n\nTo fit a Random Forest model that predicts the soil pH in the top 10cm, we are looking at different model setups. These setups always train a Random Forest model but differ in the complexity that we intentionally add to improve the final model. If you need a recap on Random Forests, have a look at the introduction given in AGDS 1.\n\n### Basic model\n\nLet's start with the basic model, where we use the {ranger} package with the pre-defined hyperparameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ranger() crashes when using tibbles, so we are using the\n# base R notation to enter the data\n\nrf_basic <- ranger::ranger( \n  y = data_cal[, response],   # Response variable\n  x = data_cal[, predictors], # Predictor variables\n  seed = 42,                  # Specify the seed for randomization to reproduce the same model again\n  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training\n\n# Print a summary of fitted model\nrf_basic |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRanger result\n\nCall:\n ranger::ranger(y = data_cal[, response], x = data_cal[, predictors],      seed = 42, num.threads = parallel::detectCores() - 1) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      605 \nNumber of independent variables:  98 \nMtry:                             9 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.2804814 \nR squared (OOB):                  0.5252541 \n```\n:::\n:::\n\n\n::: callout-tip\n## Predicting Categories with Random Forests\n\nIf our response variable was a categorical and not a continuous variable, we would have to set the argument `probability = TRUE`. The output would then be a probability map from 0-100%.\n:::\n\nAlthough we only used the pre-defined parameters, we already get a fairly good out-of-bag (OOB) $R^2$ of 0.53 and a MSE of 0.28. Let's see how we can improve our model further.\n\n### Model with weights\n\nSometimes we know that a subset of our dataset is more trustworthy than the rest. For example, when you are using a gap-filling technique to interpolate data, that gap-filled data is less trustworthy than the actually measured data. Informing the model algorithm that it should weigh certain data entries more than other can change the importance of variables and the final model performance.\n\nIn our dataset, we have information on whether the pH values were measured in the field - which is less precise - or in the lab. Also, we have to different lab methods. All of this information is held in the suffix of the `timeset` variable, so let's assign weights according on the quality of the pH data as follows: `1` for CaCl$_2$ lab measurement (no suffix), `0.9` for pedotransfer from another lab method (suffix `_ptf`), and `0.7` for field data (suffix `_field`). For this, we create a weight-matching vector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweights <- \n  data_cal |>\n  dplyr::mutate(\n    # Create a new variable 'weight' which holds only 1's\n    w = 1,\n    # Check the suffix in each row and if true, give a new weight. If false, keep the old weight.\n    w = ifelse(stringr::str_detect(timeset, \"_field\"), 0.9, w),\n    w = ifelse(stringr::str_detect(timeset, \"_ptf\"), 0.7, w)\n  )\n\n# Quality check if everything worked:\nset.seed(42)\n\nweights |> \n  dplyr::select(timeset, w) |> \n  dplyr::slice_sample(n = 8) |>   # Pick 8 random rows\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|timeset          |   w|\n|:----------------|---:|\n|d1979_2010       | 1.0|\n|d1968_1974_field | 0.9|\n|d1968_1974_field | 0.9|\n|d1968_1974_field | 0.9|\n|d1968_1974_field | 0.9|\n|d1968_1974_ptf   | 0.7|\n|d1968_1974_field | 0.9|\n|d1968_1974_field | 0.9|\n:::\n:::\n\n\nIt is always a good idea to do quality checks when wrangling! Here we see that our weight attribution code worked as expected, so we can move on to model fitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_weighted <- ranger::ranger( \n  y = data_cal[, response],      # Response variable\n  x = data_cal[, predictors],    # Predictor variables\n  case.weights = weights[, \"w\"], # Add weights to input\n  seed = 42,                     # Specify seed for randomization to reproduce the same model again\n  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training\n\n# Print a summary of fitted model\nrf_weighted |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRanger result\n\nCall:\n ranger::ranger(y = data_cal[, response], x = data_cal[, predictors],      case.weights = weights[, \"w\"], seed = 42, num.threads = parallel::detectCores() -          1) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      605 \nNumber of independent variables:  98 \nMtry:                             9 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.2874622 \nR squared (OOB):                  0.5134383 \n```\n:::\n:::\n\n\nNot much has changed compared to our previous model. We see that the $R^2$ and MSE got negligibly worse but as a trade-off we gained more trust in our model.\n\n### Variables of importance\n\nOur model has 98 variables but we have no idea if each of them should really be in the model and if we are not just fitting noise in the dataset. To investigate this issue, the `ranger()` function takes an argument called `importance`. We can set this argument either to follow the `permutation` method, whereby the algorithm randomly permutes values of each variable and measures the resulting decrease in model accuracy. A larger decrease indicates a more important variable. If the code runs slow, you can also use the faster `impurity` method (see more information [here](https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/s12859-016-0995-8.pdf)).\n\nAssessing the variable importance gives us a feeling for what variables we should keep or drop from the dataset. The ranger-model stores this information if we enter a `importance` method. The code below accesses the model's variable importance and sorts the variables with decreasing importance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's run the weighted model again but with recording the variable importance\nrf_weighted <- ranger::ranger( \n  y = data_cal[, response],     # Response variable\n  x = data_cal[, predictors],   # Predictor variables\n  case.weights = weights[, \"w\"],# Add weights to input\n  importance   = \"permutation\", # Pick permutation to calculate variable importance\n  seed = 42,                    # Specify seed for randomization to reproduce the same model again\n  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training\n\n# Writing functions here for re-usability:\nget_vi <- function(rf_model){\n  \n  # Extract the variable importance and create a long tibble\n  vi <- \n    rf_model$variable.importance |>\n    dplyr::bind_rows() |> \n    tidyr::pivot_longer(cols = dplyr::everything(), names_to = \"variable\")\n  \n  return(vi)\n}\n\nplot_vi <- function(rf_model){\n  \n  # Plot variable importance, ordered by decreasing value\n  vi <- get_vi(rf_model)\n  \n  p <- \n    vi |> \n    ggplot2::ggplot(ggplot2::aes(x = reorder(variable, value), y = value)) +\n    ggplot2::geom_bar(stat = \"identity\", fill = \"grey50\", width = 0.75) + \n    ggplot2::labs(\n      y = \"Change in OOB MSE after permutation\", \n      x = \"\",\n      title = \"Change in OOB MSE after permutation\") +\n    # ggplot2::coord_flip() +\n    ggplot2::theme_classic() +\n    ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 90, vjust = 0.5, hjust=1))\n  \n  return(p)\n}\n\nplot_vi(rf_weighted)\n```\n\n::: {.cell-output-display}\n![](03-model_fit_files/figure-html/unnamed-chunk-7-1.png){width=1152}\n:::\n:::\n\n\nWhat do we see here? The y-axis shows the decrease in model performance when the respective variable is randomly permuted and, therefore, denotes the importance of a variable. The higher the value, the stronger the effect of permutation on the model performance, the more important the variable. Also, we see that a large part of our covariates have practically no power to predict the pH and can therefore be removed from the model. But how do we determine what variable to pick for our final model? Do we want a maximum number of predictors? Do we want a set of the top n predictors that make up a certain percentage of the total variable importance?\n\nOne common option to generalize variable selection is the \"Boruta-Algorithm\", which itself is based Random Forests. In essence, the algorithm creates \"shadows\" of your original data, where each of the predictor value is randomly permuted, which destroys the predictive power of the variable. Then, the algorithm iterates over these \"shadows\" and assess for each variable whether its permutation has a substantial effect on the model performance or not. E.g., if a model trained on a variable's shadow performs constantly worse than when trained on the original values, that variable is assessed as important. The algorithm categorizes all variables into \"to reject\", \"to consider\", and \"to keep\". Luckily, we do not have to write this algorithm ourselves but can use the {Boruta} package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nbor <- \n  Boruta::Boruta(\n    y = data_cal[, response], \n    x = data_cal[, predictors],\n    maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long\n    num.threads = parallel::detectCores()-1)\n\n# Plot variable importance, the Boruta-output can be directly fed into base R plot()\npar(oma = c(8,3,2,2)) # enlarge plot below for long variable labels\nplot(bor, \n     xlab = \"\", \n     ylab = \"Importance\",\n     las = 2,\n     )\n```\n\n::: {.cell-output-display}\n![](03-model_fit_files/figure-html/unnamed-chunk-8-1.png){width=1152}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check whether the most important variables from Boruta-Algorithm are similar as the\n# important variables from the weighted Random Forest model\nbor_top10 <- \n  Boruta::attStats(bor) |> \n  tibble::rownames_to_column() |> \n  dplyr::arrange(dplyr::desc(meanImp)) |> \n  dplyr::slice_head(n = 10) |> \n  dplyr::pull(rowname)\n\nvi <- get_vi(rf_weighted)\nvi_top10 <- \n  vi |> \n  dplyr::slice_head(n = 10) |> \n  dplyr::pull(variable)\n\ncbind(vi_top10, bor_top10) |> \n  knitr::kable(col.names = c(\"RF Top 10\", \"Boruta Top 10\"))\n```\n\n::: {.cell-output-display}\n|RF Top 10      |Boruta Top 10 |\n|:--------------|:-------------|\n|timeset        |timeset       |\n|x              |y30           |\n|y              |y             |\n|be_gwn25_hdist |mt_rr_y       |\n|be_gwn25_vdist |y60           |\n|cindx10_25     |x             |\n|cindx50_25     |x30           |\n|geo500h1id     |mt_tt_y       |\n|geo500h3id     |x60           |\n|lgm            |Se_TWI2m_s15  |\n:::\n:::\n\n\nWe see that apart from the `timeset` variable, the variable importance calculated by the Boruta-Algorithm differs quite a bit, compared to the simple variable importance assessment built into the `ranger()` function. To move forward, let's only keep variables that were classified as \"to keep\" or \"to consider\" by the Boruta-Algorithm.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictors_bor <- \n  bor$finalDecision |> \n  as.data.frame(nm = \"assessment\") |> \n  tibble::rownames_to_column(var = \"variable\") |> \n  dplyr::filter(assessment != \"Rejected\") |> \n  dplyr::pull(variable)\n```\n:::\n\n\n### Hyperparameter Tuning\n\nA Random Forest model has multiple hyperparameters that we can tune to improve the model performance. Commonly, we are tuning the numbers of variables that are considered at each node `mtry` and the minimum number of observations in a leaf node `min.node.size`. To facilitate the tuning routine, we are using the {caret} package, which is a wrapper for many different statistical models. Also, we are adding a 5-fold cross-validation to improve model performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For mtry, we pick an equally spaced vector from 1 to p with a length of 5 \n# For min.node.size, we pick a a number between 1 and 9\ntune_grid <- \n  expand.grid(\n    splitrule = \"variance\", # Keep split rule the same\n    min.node.size = seq(1, 10, 2),\n    mtry          = ceiling(seq(1, length(predictors), length.out = 5))\n    )\n\ntune_grid |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|splitrule | min.node.size| mtry|\n|:---------|-------------:|----:|\n|variance  |             1|    1|\n|variance  |             3|    1|\n|variance  |             5|    1|\n|variance  |             7|    1|\n|variance  |             9|    1|\n|variance  |             1|   26|\n|variance  |             3|   26|\n|variance  |             5|   26|\n|variance  |             7|   26|\n|variance  |             9|   26|\n|variance  |             1|   50|\n|variance  |             3|   50|\n|variance  |             5|   50|\n|variance  |             7|   50|\n|variance  |             9|   50|\n|variance  |             1|   74|\n|variance  |             3|   74|\n|variance  |             5|   74|\n|variance  |             7|   74|\n|variance  |             9|   74|\n|variance  |             1|   98|\n|variance  |             3|   98|\n|variance  |             5|   98|\n|variance  |             7|   98|\n|variance  |             9|   98|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model training with cv\nrf_caret_cv <- \n    caret::train(\n        y = data_cal[, response], \n        x = data_cal[, predictors_bor],\n        # case.weights = weights[, \"w\"],\n        tuneGrid = tune_grid,\n        method = \"ranger\",\n        trControl = caret::trainControl(method = \"cv\", number = 5)\n    )\n\nrf_caret_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Forest \n\n605 samples\n 57 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 484, 484, 483, 485, 484 \nResampling results across tuning parameters:\n\n  min.node.size  mtry  RMSE       Rsquared   MAE      \n  1               1    0.5496691  0.5082110  0.4416633\n  1              26    0.5080837  0.5667532  0.4085002\n  1              50    0.5083564  0.5652961  0.4087318\n  1              74          NaN        NaN        NaN\n  1              98          NaN        NaN        NaN\n  3               1    0.5520794  0.5043353  0.4426002\n  3              26    0.5086833  0.5660987  0.4096449\n  3              50    0.5065478  0.5688146  0.4072980\n  3              74          NaN        NaN        NaN\n  3              98          NaN        NaN        NaN\n  5               1    0.5525488  0.5042816  0.4435890\n  5              26    0.5079319  0.5668622  0.4100156\n  5              50    0.5084420  0.5651644  0.4093447\n  5              74          NaN        NaN        NaN\n  5              98          NaN        NaN        NaN\n  7               1    0.5539034  0.5016743  0.4450918\n  7              26    0.5088478  0.5656766  0.4089423\n  7              50    0.5088371  0.5646783  0.4095499\n  7              74          NaN        NaN        NaN\n  7              98          NaN        NaN        NaN\n  9               1    0.5560706  0.5000875  0.4469837\n  9              26    0.5118963  0.5600316  0.4121397\n  9              50    0.5085219  0.5650709  0.4093291\n  9              74          NaN        NaN        NaN\n  9              98          NaN        NaN        NaN\n\nTuning parameter 'splitrule' was held constant at a value of variance\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 50, splitrule = variance\n and min.node.size = 3.\n```\n:::\n\n```{.r .cell-code}\n# Model training with weights\n# rf_caret_w <- \n#     caret::train(\n#       y = data_cal[, response], \n#       x = data_cal[, predictors_bor],\n#       case.weights = weights[, \"w\"],\n#       tuneGrid = tune_grid,\n#       method = \"ranger\",\n#       # trControl = caret::trainControl(method = \"cv\", number = 5)\n#   )\n# rf_caret_w$bestTune\n```\n:::\n\n\n::: callout-note\n## CV with weights\n\nUnfortuntately, {caret} handles the weights wrong during CV, which throws an [error](#0) when trying to use weights alongside CV. As a work-around, one could write the CV by hand and run caret with the weights as input. As we aim for generalizability of our model, we are preferring the cross-validated over the weighted model.\n:::\n\nFrom this model output, we can see that the best model had a `mtry = 31`, a `min.node.size = 10`. Since we are going to use functions from the {ranger} package for the model interpretation in the next Chapter, we have to first rerun our final model with `range()` and then save all relevant data for further analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_final <- \n  ranger::ranger( \n    y = data_cal[, response],         # Response variable\n    x = data_cal[, predictors_bor],   # Predictor variables\n    case.weights = weights[, \"w\"],# Add weights to input\n    importance   = \"permutation\", # Pick permutation to calculate variable importance\n    mtry = rf_caret_cv$finalModel$mtry,\n    min.node.size = rf_caret_cv$finalModel$min.node.size,\n    splitrule = \"variance\",\n    seed = 42,                    # Specify seed for randomization to reproduce the same model again\n    num.threads = parallel::detectCores() - 1 # Use all but one CPU core for quick model training\n  )\n\nprint(rf_final)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRanger result\n\nCall:\n ranger::ranger(y = data_cal[, response], x = data_cal[, predictors_bor],      case.weights = weights[, \"w\"], importance = \"permutation\",      mtry = rf_caret_cv$finalModel$mtry, min.node.size = rf_caret_cv$finalModel$min.node.size,      splitrule = \"variance\", seed = 42, num.threads = parallel::detectCores() -          1) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      605 \nNumber of independent variables:  57 \nMtry:                             50 \nTarget node size:                 3 \nVariable importance mode:         permutation \nSplitrule:                        variance \nOOB prediction error (MSE):       0.2595114 \nR squared (OOB):                  0.5607482 \n```\n:::\n:::\n\n\n## Model interpretation\n\n### Variable importance\n\n{Similar as already done above for variable selection.}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_vi(rf_final)\n```\n\n::: {.cell-output-display}\n![](03-model_fit_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### Partial dependence plots\n\nPartial dependence plots (PDP) show the marginal effect that a given predictor has on the response, when the effects of all other predictors are accounted for. We know that not all variables are very influential, so we look only at the top give variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the names of the 5 most important variables\n\n# Top n variables to create PDPs for\ntop_n <- 5\n\n# Get top variables\ntop_vars <- \n  get_vi(rf_final) |> \n  dplyr::slice_head(n = top_n) |> \n  dplyr::pull(variable)\n\n# Create a small loop to extract the predictor values and y_hat\n# Create separate df for numerical and for categorical variables\npdp_data_num <- tibble::tibble()\npdp_data_cat <- tibble::tibble()\n\nfor (i_var in top_vars) {\n  i_data <- \n    pdp::partial(rf_final, \n                 pred.var = i_var, # select one variable you are interessted in \n                 train = data_cal[, c(response, predictors_bor)]) |> \n    tidyr::pivot_longer(cols = i_var, names_to = \"variable\", values_to = \"x\")\n  \n  if (is.character(i_data[['x']])) {\n    pdp_data_cat <- rbind(pdp_data_cat, i_data)  \n    \n  } else {\n    pdp_data_num <- rbind(pdp_data_num, i_data)  \n    \n  }\n}\n\n# Plot numerical variables\npdp_data_num |> \n  ggplot2::ggplot() +\n  ggplot2::geom_line(aes(x = x, y = yhat)) +\n  ggplot2::facet_wrap(~variable, scales = \"free_x\", nrow = 1) +\n  ggplot2::theme_linedraw() +\n  ggplot2::theme(axis.text.x = element_text(angle = 45, hjust = 1),\n                 panel.grid.minor.x = element_blank()) +\n  ggplot2::labs(y = \"Response [unit of response]\",\n                x = NULL)\n```\n\n::: {.cell-output-display}\n![](03-model_fit_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot categorical variables\npdp_data_cat |> \n  ggplot2::ggplot() +\n  ggplot2::geom_point(aes(x = x, y = yhat)) +\n  ggplot2::facet_wrap(~variable, scales = \"free_x\", nrow = 1) +\n  ggplot2::theme_linedraw() +\n  ggplot2::theme(axis.text.x = element_text(angle = 45, hjust = 1),\n                 panel.grid.minor.x = element_blank()) +\n  ggplot2::labs(y = \"Response [unit of response]\",\n                x = NULL)\n```\n\n::: {.cell-output-display}\n![](03-model_fit_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n### Save outputs for evaluation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save all relevant data for model interpretation\nsaveRDS(rf_final,                   \n        here::here(\"data/rf_for_ph0-10.rds\"))\n\nsaveRDS(data_cal[, c(response, predictors_bor)],\n        here::here(\"data/cal_for_ph0-10.rds\"))\n\nsaveRDS(data_val[, c(response, predictors_bor)],\n        here::here(\"data/val_for_ph0-10.rds\"))\n\nsaveRDS(weights[, \"w\"],\n        here::here(\"data/weights_for_ph0-10.rds\"))\n```\n:::\n\n\n<!-- > TODO: Think about whether we should center and scale the data. Is not needed for RF per se but it could help with model interpretability and variable importance assessment. -->\n\n<!-- > TODO: Add interpretation of variable importance and of PDP plots -->\n",
    "supporting": [
      "03-model_fit_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}