{
  "hash": "0184b0ae38ed4d6afe387edd8b57df4e",
  "result": {
    "markdown": "# Train a Random Forest {#sec-modelfit}\n\nIn this chapter, we are going to use the data prepared in @sec-dataprep, train a Random Forest model, and evaluate the variable importance of the different predictors used in the model. To recap the basics of the Random Forest algorithm and its implementation and use in R, head over to [Chapter 11 of AGDS book](https://geco-bern.github.io/agds/randomforest.html).\n\n## Load data\n\nIn the previous Chapter, we created a dataframe that holds information on the soil sampling locations and the covariates that we extracted for these positions. Let's load this datafarme into our environment\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_clean <- readRDS(here::here(\"data/bern_sampling_locations_with_covariates.rds\"))\n\nhead(data_clean) |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|site_id_unique |timeset        |       x|       y|dataset     |dclass   | waterlog.30| waterlog.50| waterlog.100|  ph.0.10| ph.10.30| ph.30.50| ph.50.100| be_gwn25_hdist| be_gwn25_vdist| cindx10_25|  cindx50_25|geo500h1id |geo500h3id |lgm |       lsf|   mrrtf25|  mrvbf25|  mt_gh_y|  mt_rr_y| mt_td_y| mt_tt_y| mt_ttvar|     NegO|     PosO|  protindx| Se_alti2m_std_50c|   Se_conv2m| Se_curv25m| Se_curv2m_fmean_50c| Se_curv2m_fmean_5c| Se_curv2m_s60| Se_curv2m_std_50c| Se_curv2m_std_5c|  Se_curv2m| Se_curv50m|  Se_curv6m| Se_curvplan25m| Se_curvplan2m_fmean_50c| Se_curvplan2m_fmean_5c| Se_curvplan2m_s60| Se_curvplan2m_s7| Se_curvplan2m_std_50c| Se_curvplan2m_std_5c| Se_curvplan2m| Se_curvplan50m| Se_curvprof25m| Se_curvprof2m_fmean_50c| Se_curvprof2m_fmean_5c| Se_curvprof2m_s60| Se_curvprof2m_s7| Se_curvprof2m_std_50c| Se_curvprof2m_std_5c| Se_curvprof2m| Se_curvprof50m| Se_diss2m_50c| Se_diss2m_5c| Se_e_aspect25m| Se_e_aspect2m_5c| Se_e_aspect2m| Se_e_aspect50m| Se_MRRTF2m| Se_MRVBF2m| Se_n_aspect2m_50c| Se_n_aspect2m_5c| Se_n_aspect2m| Se_n_aspect50m| Se_n_aspect6m| Se_NO2m_r500| Se_PO2m_r500| Se_rough2m_10c| Se_rough2m_5c| Se_rough2m_rect3c| Se_SAR2m|  Se_SCA2m| Se_slope2m_fmean_50c| Se_slope2m_fmean_5c| Se_slope2m_s60| Se_slope2m_s7| Se_slope2m_std_50c| Se_slope2m_std_5c| Se_slope2m| Se_slope50m| Se_slope6m| Se_toposcale2m_r3_r50_i10s| Se_tpi_2m_50c| Se_tpi_2m_5c| Se_tri2m_altern_3c| Se_tsc10_2m| Se_TWI2m_s15| Se_TWI2m_s60|  Se_TWI2m| Se_vrm2m_r10c|  Se_vrm2m| terrTextur|  tsc25_18| tsc25_40|   vdcn25|vszone |\n|:--------------|:--------------|-------:|-------:|:-----------|:--------|-----------:|-----------:|------------:|--------:|--------:|--------:|---------:|--------------:|--------------:|----------:|-----------:|:----------|:----------|:---|---------:|---------:|--------:|--------:|--------:|-------:|-------:|--------:|--------:|--------:|---------:|-----------------:|-----------:|----------:|-------------------:|------------------:|-------------:|-----------------:|----------------:|----------:|----------:|----------:|--------------:|-----------------------:|----------------------:|-----------------:|----------------:|---------------------:|--------------------:|-------------:|--------------:|--------------:|-----------------------:|----------------------:|-----------------:|----------------:|---------------------:|--------------------:|-------------:|--------------:|-------------:|------------:|--------------:|----------------:|-------------:|--------------:|----------:|----------:|-----------------:|----------------:|-------------:|--------------:|-------------:|------------:|------------:|--------------:|-------------:|-----------------:|--------:|---------:|--------------------:|-------------------:|--------------:|-------------:|------------------:|-----------------:|----------:|-----------:|----------:|--------------------------:|-------------:|------------:|------------------:|-----------:|------------:|------------:|---------:|-------------:|---------:|----------:|---------:|--------:|--------:|:------|\n|4_26-In-005    |d1968_1974_ptf | 2571994| 1203001|validation  |poor     |           0|           0|            1| 6.071733| 6.227780| 7.109235|  7.214589|      234.39087|      1.2986320|  -10.62191|  -6.9658718|6          |0          |7   | 0.0770846| 0.0184651| 4.977099| 1316.922| 9931.120|      58|      98|      183| 1.569110| 1.534734| 0.0159717|         0.3480562| -40.5395088| -0.0014441|          -0.0062570|          0.0175912|     0.0002296|         2.9204133|        1.1769447| -1.9364884|  0.0031319| -0.5886537|     -0.0042508|              -0.0445323|             -0.0481024|        -0.0504083|       -0.1655090|             1.5687343|            0.6229440|    -1.0857303|      0.0007920|     -0.0028067|              -0.0382753|             -0.0656936|        -0.0506380|       -0.0732220|             1.6507173|            0.7082230|     0.8507581|     -0.0023399|     0.3934371|    0.1770810|     -0.9702092|       -0.7929600|    -0.5661940|     -0.9939429|   5.930607|   6.950892|        -0.2840056|       -0.6084610|    -0.2402939|     -0.0577110|    -0.7661251|     1.562085|     1.548762|      0.3228087|     0.2241062|         0.2003846| 4.000910| 16.248077|            0.9428899|           0.6683306|      0.9333237|     0.7310556|          0.8815832|         0.3113754|  1.1250136|   0.3783818|  0.5250366|                          0|    -0.0940372|   -0.0583917|          10.319408|   0.4645128|    0.0032796|    0.0049392| 0.0011592|      0.000125| 0.0002450|  0.6248673| 0.3332805| 1.784737| 65.62196|6      |\n|4_26-In-006    |d1974_1978     | 2572149| 1202965|calibration |poor     |           0|           1|            1| 6.900000| 6.947128| 7.203502|  7.700000|      127.41681|      1.7064546|  -10.87862| -11.8201790|6          |0          |7   | 0.0860347| 0.0544361| 4.975796| 1317.000| 9931.672|      58|      98|      183| 1.568917| 1.533827| 0.0204794|         0.1484705|  19.0945148| -0.0190294|           0.0021045|          0.0221433|     0.0000390|         3.8783867|        4.3162045|  2.1377332| -0.0171786|  0.1278165|     -0.0119618|              -0.0501855|             -0.3270764|        -0.1004921|       -0.5133076|             2.0736780|            2.2502327|    -0.3522736|     -0.0073879|      0.0070676|              -0.0522900|             -0.3492197|        -0.1005311|       -0.4981292|             2.1899190|            2.4300070|    -2.4900069|      0.0097907|     0.4014700|    0.7360508|      0.5683194|        0.8753148|    -0.3505180|      0.3406741|   5.984921|   6.984581|        -0.5732749|        0.4801802|     0.4917848|     -0.4550385|     0.7722272|     1.543384|     1.558683|      0.2730940|     0.2489859|         0.2376962| 4.001326|  3.357315|            1.0895698|           0.9857153|      1.0231543|     1.0398037|          1.0152543|         0.5357812|  1.3587183|   0.0645478|  0.5793087|                          0|    -0.0014692|    0.0180000|          12.603136|   0.5536283|    0.0070509|    0.0067992| 0.0139006|      0.000300| 0.0005389|  0.7573612| 0.3395441| 1.832904| 69.16074|6      |\n|4_26-In-012    |d1974_1978     | 2572937| 1203693|calibration |moderate |           0|           1|            1| 6.200000| 6.147128| 5.603502|  5.904355|      143.41533|      0.9372618|   22.10210|   0.2093917|6          |0          |7   | 0.0737963| 3.6830916| 4.986864| 1315.134| 9935.438|      58|      98|      183| 1.569093| 1.543057| 0.0048880|         0.1112066|  -9.1396294|  0.0039732|           0.0009509|          0.0431735|     0.0034232|         0.7022317|        0.4170935| -0.4178924| -0.0026431| -0.0183221|      0.0015183|              -0.0079620|              0.0053904|        -0.0091239|       -0.0110896|             0.3974485|            0.2292406|    -0.2168447|     -0.0013561|     -0.0024548|              -0.0089129|             -0.0377831|        -0.0125471|       -0.0052359|             0.4158890|            0.2700820|     0.2010477|      0.0012870|     0.6717541|    0.4404107|     -0.6987815|       -0.3866692|    -0.1960597|     -0.7592779|   5.953919|   6.990917|        -0.3006475|       -0.9221049|    -0.9633239|     -0.3257418|    -0.9502072|     1.565405|     1.563151|      0.2305476|     0.2182523|         0.1434273| 4.000320| 11.330072|            0.5758902|           0.5300468|      0.5107915|     0.5744110|          0.4975456|         0.2001768|  0.7160403|   0.1311051|  0.4620202|                          0|     0.0340407|   -0.0145804|           7.100000|   0.4850160|    0.0021498|    0.0017847| 0.0011398|      0.000000| 0.0000124|  0.7978453| 0.4455501| 1.981526| 63.57096|6      |\n|4_26-In-014    |d1974_1978     | 2573374| 1203710|validation  |well     |           0|           0|            0| 6.600000| 6.754607| 7.200000|  7.151129|      165.80418|      0.7653937|  -20.11569|  -7.7729993|6          |0          |7   | 0.0859686| 0.0075817| 5.285522| 1315.160| 9939.923|      58|      98|      183| 1.569213| 1.542792| 0.0064054|         0.3710849|  -0.9318936| -0.0371234|           0.0029348|         -0.1056513|     0.0127788|         1.5150748|        0.2413423| -0.0289909|  0.0020990| -0.0706228|     -0.0113604|              -0.0301961|             -0.0346193|        -0.0273140|       -0.0343277|             0.8245047|            0.1029889|    -0.0272214|     -0.0041158|      0.0257630|              -0.0331309|              0.0710320|        -0.0400928|        0.0529446|             0.8635767|            0.1616543|     0.0017695|     -0.0062147|     0.4988544|    0.4217250|     -0.8485889|       -0.8657616|    -0.8836724|     -0.8993938|   4.856076|   6.964162|        -0.5735765|       -0.4998477|    -0.4677161|     -0.4121092|    -0.4782534|     1.562499|     1.562670|      0.3859352|     0.2732429|         0.1554769| 4.000438| 42.167496|            0.8873205|           0.8635756|      0.9015982|     0.8518201|          0.5767300|         0.2149791|  0.8482135|   0.3928713|  0.8432562|                          0|     0.0686932|   -0.0085602|           8.303085|   0.3951114|    0.0008454|    0.0021042| 0.0000000|      0.000100| 0.0000857|  0.4829135| 0.4483251| 2.113142| 64.60535|6      |\n|4_26-In-015    |d1968_1974_ptf | 2573553| 1203935|validation  |moderate |           0|           0|            1| 6.272715| 6.272715| 6.718392|  7.269008|       61.39244|      1.0676192|  -55.12566| -14.0670462|6          |0          |7   | 0.0650000| 0.0007469| 5.894688| 1315.056| 9942.032|      58|      98|      183| 1.570359| 1.541979| 0.0042235|         0.3907509|   4.2692256|  0.0378648|           0.0022611|         -0.1020419|     0.0161510|         3.6032522|        1.8169731|  0.6409346|  0.0346340|  0.0476020|      0.0378154|              -0.0179657|             -0.0137853|        -0.0146946|        0.0060875|             1.4667766|            0.9816071|     0.2968794|      0.0337645|     -0.0000494|              -0.0202268|              0.0882566|        -0.0308456|        0.0929077|             2.6904552|            1.0218329|    -0.3440553|     -0.0008695|     0.6999696|    0.3944107|     -0.8918364|       -0.8864348|    -0.7795515|     -0.4249992|   4.130917|   6.945287|         0.4304937|        0.4614536|     0.5919228|      0.6559467|     0.4574654|     1.550528|     1.562685|      0.4330348|     0.3299487|         0.1889674| 4.000948|  5.479310|            1.8937486|           1.2098556|      1.5986075|     1.2745584|          2.7759163|         0.5375320|  1.2301254|   0.3582314|  1.1426100|                          0|     0.3005829|    0.0061576|          10.110727|   0.5134069|    0.0043268|    0.0045225| 0.0054557|      0.000200| 0.0002062|  0.6290755| 0.3974232| 2.080674| 61.16533|6      |\n|4_26-In-016    |d1968_1974_ptf | 2573310| 1204328|calibration |poor     |           0|           0|            1| 6.272715| 6.160700| 5.559031|  5.161655|      310.05014|      0.1321367|  -17.16055| -28.0693741|6          |0          |7   | 0.0731646| 0.0128017| 5.938320| 1315.000| 9940.597|      58|      98|      183| 1.569434| 1.541606| 0.0040683|         0.1931891|  -0.1732794| -0.1602274|          -0.0035833|         -0.1282881|     0.0003549|         1.5897882|        0.8171870|  0.0318570| -0.0123340|  0.0400775|     -0.0813964|              -0.0049875|              0.0320331|        -0.0049053|        0.0374298|             0.7912259|            0.3455668|     0.0100844|     -0.0059622|      0.0788309|              -0.0014042|              0.1603212|        -0.0052602|        0.0867119|             1.0207798|            0.6147888|    -0.0217726|      0.0063718|     0.3157751|    0.5292308|     -0.8766075|        0.5905659|     0.8129975|      0.1640853|   2.030315|   6.990967|         0.6325440|        0.8054439|     0.5820994|      0.7448481|     0.6081498|     1.563066|     1.552568|      0.3688371|     0.2607146|         0.1763995| 4.000725| 13.499996|            1.0418727|           0.8515157|      1.2106605|     0.8916541|          1.2163279|         0.4894866|  1.0906221|   0.2049688|  0.7156029|                          0|    -0.0910767|    0.0034276|           9.574804|   0.3864355|    0.0001476|    0.0003817| 0.0000000|      0.000525| 0.0001151|  0.6997021| 0.4278295| 2.041467| 55.78354|6      |\n:::\n:::\n\n\n## Preparations\n\nBefore we can fit the model, we have to specify a few settings. First, we have to specify our response and predictor variables. Then, we have to split our dataset into a calibration and a validation set. Random Forest models cannot deal with `NA` values, so we have to remove these from our calibration set. \n\nIn the dataset we work with here, the data splitting into training and testing sets is defined by the column `dataset`. Usually, this is not the case and we split the data ourselves. Have a look at the [section in AGDS Book on data splitting](https://geco-bern.github.io/agds/supervisedmli.html#data-splitting).\n\nAs predictors we use all the variables for which we have data with spatial coverage - the basis for spatial upscaling. We extracted these data in @sec-dataprep from geospatial files and the data frame was constructed by `cbind()`, where columns number 14-104 contain the covariates data, extracted from the geospatial files. See @sec-variables for a description of the variables. We use all of them here as predictors for the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify response: The pH in the top 10cm\nresponse <- \"ph.0.10\"\n\n# Specify predictors: Remove soil sampling information\npredictors <- names(data_clean)[14:ncol(data_clean)]\n\ncat(\"The response is:\", response,\n    \"\\nThe predictors are:\", paste0(predictors[1:8], sep = \", \"), \"...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe response is: ph.0.10 \nThe predictors are: be_gwn25_hdist,  be_gwn25_vdist,  cindx10_25,  cindx50_25,  geo500h1id,  geo500h3id,  lgm,  lsf,  ...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split dataset into calibration and validation\ndata_cal <- data_clean |> dplyr::filter(dataset == \"calibration\")\ndata_val <- data_clean |> dplyr::filter(dataset == \"validation\")\n\n# Filter out any NA to avoid error when running a Random Forest\ndata_cal <- data_cal |> tidyr::drop_na()\ndata_val <- data_val |> tidyr::drop_na()\n\n# A little bit of verbose output:\nn_tot <- nrow(data_cal) + nrow(data_val)\n\nperc_cal <- (nrow(data_cal) / n_tot) |> round(2) * 100\nperc_val <- (nrow(data_val) / n_tot) |> round(2) * 100\n\ncat(\"For model training, we have a calibration / validation split of: \",\n    perc_cal, \"/\", perc_val, \"%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor model training, we have a calibration / validation split of:  75 / 25 %\n```\n:::\n:::\n\n\nAlright, this looks all good. We have our response and predictor variables saved for easy access later on and the 75/25% split of calibration and validation data looks good, too. We can now move on to model fitting.\n\n## Model training\n\nThe modelling task is to predict the soil pH in the top 10 cm. Let's start using the default hyperparameters used by `ranger::ranger()`.\n\n::: callout-tip\nHave a look at the values of the defaults by entering. `?ranger::ranger` in your console and study the function documentation. \n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ranger() crashes when using tibbles, so we are using the\n# base R notation to enter the data\n\nrf_basic <- ranger::ranger( \n  y = data_cal[, response],   # Response variable\n  x = data_cal[, predictors], # Predictor variables\n  seed = 42,                  # Specify the seed for randomization to reproduce the same model again\n  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training\n\n# Print a summary of fitted model\nprint(rf_basic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRanger result\n\nCall:\n ranger::ranger(y = data_cal[, response], x = data_cal[, predictors],      seed = 42, num.threads = parallel::detectCores() - 1) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      605 \nNumber of independent variables:  91 \nMtry:                             9 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.3223462 \nR squared (OOB):                  0.4543932 \n```\n:::\n:::\n\n\n::: callout-tip\n## Predicting categories with Random Forests\n\nIf our response variable was a categorical and not a continuous variable, we would have to set the argument `probability = TRUE`. The output would then be a probability map from 0-100%.\n:::\n\nAlthough we only used the pre-defined parameters, we already get a fairly good out-of-bag (OOB) $R^2$ of 0.45 and a MSE of 0.32 pH units. See [here](https://geco-bern.github.io/agds/randomforest.html#out-of-bag-prediction) for more background on OOB error estimation with Random Forests.\n\n<!-- ### Model with weights -->\n\n<!-- Sometimes we know that a subset of our dataset is more trustworthy than the rest. For example, when you are using a gap-filling technique to interpolate data, that gap-filled data is less trustworthy than the actually measured data. Informing the model algorithm that it should weigh certain data entries more than others can affect the final model performance. -->\n\n<!-- In our dataset, we have information on whether the pH values were measured in the field - which is less precise - or in the lab. Also, two different lab methods are indicated in the data. All of this information is held in the suffix of the `timeset` variable, so let's assign weights according on the quality of the pH data as follows: `1` for CaCl$_2$ lab measurement (no suffix), `0.9` for pedotransfer from another lab method (suffix `_ptf`), and `0.7` for field data (suffix `_field`). For this, we create a weight-matching vector: -->\n\n<!-- ```{r} -->\n<!-- weights <-  -->\n<!--   data_cal |> -->\n<!--   dplyr::mutate( -->\n<!--     # Create a new variable 'weight' which holds only 1's -->\n<!--     w = 1, -->\n<!--     # Check the suffix in each row and if true, give a new weight. If false, keep the old weight. -->\n<!--     w = ifelse(stringr::str_detect(timeset, \"_field\"), 0.9, w), -->\n<!--     w = ifelse(stringr::str_detect(timeset, \"_ptf\"), 0.7, w) -->\n<!--   ) -->\n\n<!-- # Quality check if everything worked: -->\n<!-- set.seed(42) -->\n\n<!-- weights |>  -->\n<!--   dplyr::select(timeset, w) |>  -->\n<!--   dplyr::slice_sample(n = 8) |>   # Pick 8 random rows -->\n<!--   knitr::kable() -->\n<!-- ``` -->\n\n<!-- It is always a good idea to do quality checks when wrangling! Here we see that our weight attribution code worked as expected, so we can move on to model fitting. -->\n\n<!-- ```{r} -->\n<!-- rf_weighted <- ranger::ranger(  -->\n<!--   y = data_cal[, response],      # Response variable -->\n<!--   x = data_cal[, predictors],    # Predictor variables -->\n<!--   case.weights = weights[, \"w\"], # Add weights to input -->\n<!--   seed = 42,                     # Specify seed for randomization to reproduce the same model again -->\n<!--   num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training -->\n\n<!-- # Print a summary of fitted model -->\n<!-- rf_weighted |> print() -->\n<!-- ``` -->\n\n<!-- Not much has changed compared to our previous model. We see that the $R^2$ and MSE got negligibly worse but as a trade-off we gained more trust in our model. -->\n\nThis is the step at which you may want to reduce the number of predictors to avoid collinearity and the risk of overfitting. You may also want to optimize the hyperparameters for improving the model performance and generalisability. Different hyperparameter specifications of the Random Forest model that control the model complexity may be compared. A simple way to do that is to use the {caret} R package which provides machine learning wrapper functions for hyperparameter tuning (among many more functionalities). Its use in combination with Random Forest is demonstrated in [Chapter 11 of AGDS book](https://geco-bern.github.io/agds/randomforest.html). Reducing the number of predictors and retaining only the most important ones is important for obtaining robust model generalisability and is approached by what is shown below.\n\n## Variable importance\n\nOur model has 91 variables, but we don't know anything about their role in influencing the model predictions and how important they are for achieving good predictions. `ranger::ranger()` provides a built-in functionality for quantifying  variable importance based on the OOB-error. This functionality can be controlled with the argument `importance`. When set to `'permutation'`, the algorithm randomly permutes values of each variable, one at a time, and measures the importance as the resulting decrease in the OOB prediction skill of each decision tree within the Random Forest and returns the average across importances of all decision trees. Note that this is a model-specific variable importance quantification method. In [AGDS Book Chapter 12](https://geco-bern.github.io/agds/interpretableml.html#variable-importance), you have learned about a model model-agnostic method.\n\nThe model object returned by the `ranger()` function stores the variable importance information. The code below accesses this information and sorts the predictor variables with decreasing importance. If the code runs slow, you can also use the faster `impurity` method (see more information [here](https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/s12859-016-0995-8.pdf)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's run the basic model again but with recording the variable importance\nrf_basic <- ranger::ranger( \n  y = data_cal[, response],     # Response variable\n  x = data_cal[, predictors],   # Predictor variables\n  importance   = \"permutation\", # Pick permutation to calculate variable importance\n  seed = 42,                    # Specify seed for randomization to reproduce the same model again\n  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training\n\n# Extract the variable importance and create a long tibble\nvi_rf_basic <- rf_basic$variable.importance |>\n  dplyr::bind_rows() |> \n  tidyr::pivot_longer(cols = dplyr::everything(), names_to = \"variable\")\n\n# Plot variable importance, ordered by decreasing value\ngg <- vi_rf_basic |> \n  ggplot2::ggplot(ggplot2::aes(x = reorder(variable, value), y = value)) +\n  ggplot2::geom_bar(stat = \"identity\", fill = \"grey50\", width = 0.75) + \n  ggplot2::labs(\n    y = \"Change in OOB MSE after permutation\", \n    x = \"\",\n    title = \"Variable importance based on OOB\") +\n  ggplot2::theme_classic() +\n  ggplot2::coord_flip()\n\n# Display plot\ngg\n```\n\n::: {.cell-output-display}\n![](03-model_training_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWhat do we see here? The higher the value, the stronger the effect of permutation on the model performance, the more important the variable. The five most important variables are the following:\n\n| Importance rank  | Variable name      | Description                                             |\n|------------------|--------------------|---------------------------------------------------------|\n| 1                | mt_rr_y            | Mean annual precipitation                               |\n| 2                | mt_tt_y            | Mean annual temperature                                 |\n| 3                | mt_td_y            | Mean annual dew point temperature                       |\n| 4                | mt_gh_y            | Mean annual incoming radiation                          |\n| 5                | be_gwn25_vdist     | Horizontal distance to water body at 25m resolution     |\n\nWe find that the mean annual precipitation is by far the most important variable in determining soil pH in our model. From a soil-forming perspective, this seems plausible [@dawson77]. We further find that the four most important variables all describe climate - reflecting its important role as a soil-forming factor. Most of the remaining variables are metrics of the topography. It should also be noted that many of them may be correlated since. Some of them measure the same aspect of topography, but derived from a digital elevation model given at different spatial resolution (see @sec-variables). Due to their potential correlation, dropping one of the affected variables from the model may thus not lead to a strong deterioration of the model skill as its (correlated) information is still contained in the remaining variables. \n\n## Variable selection\n\nThe large number of variables in our model and the tendency that many of them exhibit a low importance in comparison to the dominating few, and that they may be correlated calls for a *variable selection*. Reducing the number of predictors reduces the risk that remaining predictors are correlated. Having correlated predictors is a problem - as shown in the context of spatial upscaling by [@ludwig23]. Intuitively, this makes sense in view of the fact that if $x_i$ and $x_j$ are correlated, then, for example, $x_i$ is used for modelling its true association with the target variable, while $x_j$ can be \"spent\" to model randomly occurring covariations with the target - potentially modelling noise in the data. If this happens, overfitting will follow (see [here](https://geco-bern.github.io/agds/supervisedmli.html#overfitting)). \n\nDifferent strategies for reducing the number of predictors, while retaining model performance and improving model generalisability, exist. *Greedy search*, or *stepwise regression* are often used. Their approach is to sequentially add (stepwise *forward*) or remove (stepwise *backward*) predictors and to determine the best (complemented or reduced) set of predictors in terms of respective model performance at each step. The algorithm stops once the model starts to deteriorate or stops improving. However, it should be noted that these algorithms don't assess all possible combinations of predictors and may thus not find the \"globally\" optimal model. A stepwise regression was implemented in AGDS I as a Report Exercise (see [here](https://geco-bern.github.io/agds/regressionclassification.html#report-exercise)). \n\n::: callout-tip\nTo dig deeper into understanding how the model works, we could further investigate its partial dependence plots (see [here](https://geco-bern.github.io/agds/interpretableml.html#partial-dependence-plots)).\n:::\n\nAn alternative approach to model selection is to consider the variable importance. Predictors may be selected based on whether removing their association with the target variable (by permuting values of the predictor) deteriorates the model. Additionally, a decision criterion can be introduced for determining whether or not to retain the respective variable. This is implemented by the \"Boruta-Algorithm\" - an effective and popular approach to variable selection [@kursa10]. Boruta is available as an R package {Boruta}, is based on Random Forests, and performs a permutation of variables for determining their importance - as described in [Chapter 12 of AGDS Book](https://geco-bern.github.io/agds/interpretableml.html#variable-importance) for model-agnostic variable importance estimation. The algorithm finally categorizes variables into `\"Rejected\"`, `\"Tentative\"`, and `\"Confirmed\"`. Let's apply Boruta on our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\n# run the algorithm\nbor <- Boruta::Boruta(\n    y = data_cal[, response], \n    x = data_cal[, predictors],\n    maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long\n    num.threads = parallel::detectCores()-1)\n\n# obtain results: a data frame with all variables, ordered by their importance\ndf_bor <- Boruta::attStats(bor) |> \n  tibble::rownames_to_column() |> \n  dplyr::arrange(dplyr::desc(meanImp))\n\n# plot the importance result  \nggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), \n                             y = meanImp,\n                             fill = decision), \n                data = df_bor) +\n  ggplot2::geom_bar(stat = \"identity\", width = 0.75) + \n  ggplot2::scale_fill_manual(values = c(\"grey30\", \"tomato\", \"grey70\")) + \n  ggplot2::labs(\n    y = \"Variable importance\", \n    x = \"\",\n    title = \"Variable importance based on Boruta\") +\n  ggplot2::theme_classic() +\n  ggplot2::coord_flip()\n```\n\n::: {.cell-output-display}\n![](03-model_training_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n::: callout-tip\nDetermine the length $N$ of the vector of predictors deemed important by (`\"Confirmed\"`) Boruta and compare these \"important\" variables with the $N$ most important variables of the OOB-based variable importance estimation demonstrated above.\n:::\n\n<!-- ```{r} -->\n<!-- # Check whether the most important variables from Boruta-Algorithm are similar as the -->\n<!-- # important variables from the weighted Random Forest model -->\n<!-- bor_top10 <- Boruta::attStats(bor) |>  -->\n<!--   tibble::rownames_to_column() |>  -->\n<!--   dplyr::arrange(dplyr::desc(meanImp)) |>  -->\n<!--   dplyr::slice_head(n = 10) |>  -->\n<!--   dplyr::pull(rowname) -->\n\n<!-- vi_top10 <- vi_rf_basic |> -->\n<!--   dplyr::arrange(dplyr::desc(value)) |>  -->\n<!--   dplyr::slice_head(n = 10) |>  -->\n<!--   dplyr::pull(variable) -->\n\n<!-- cbind(vi_top10, bor_top10) |>  -->\n<!--   knitr::kable(col.names = c(\"RF Top 10\", \"Boruta Top 10\")) -->\n<!-- ``` -->\n\nFor the spatial upscaling in the context of digital soil mapping, let's retain only the variables deemed important (`\"Confirmed\"`) by the Boruta algorithm and retrain a final Random Forest model. The number of retained variables is 34.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get retained important variables\npredictors_bor <- df_bor |> \n  dplyr::filter(decision == \"Confirmed\") |>\n  dplyr::pull(rowname)\n\nlength(predictors_bor)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 33\n```\n:::\n\n```{.r .cell-code}\n# re-train Random Forest model\nrf_bor <- ranger::ranger( \n  y = data_cal[, response],       # Response variable\n  x = data_cal[, predictors_bor], # Predictor variables\n  seed = 42,                      # Specify the seed for randomization to reproduce the same model again\n  num.threads = parallel::detectCores() - 1) # Use all but one CPU core for quick model training\n```\n:::\n\n\n::: callout-tip\nCompare the skill of the models with all predictors and with the Boruta-informed reduced set of predictors. What is happening?\n:::\n\nFor this tutorial, we are happy to continue with our most basic Random Forest and save all necessary data as shown below. However, as shown with the model using weights, with the variable selection of the Boruta method, and with applying hyper-parameter tuning and cross-validation for a more robust model, there are many options to create a better model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save relevant data for model testing in the next chapter.\nsaveRDS(rf_bor,                   \n        here::here(\"data/rf_for_ph0-10.rds\"))\n\nsaveRDS(data_cal[, c(response, predictors_bor)],\n        here::here(\"data/cal_for_ph0-10.rds\"))\n\nsaveRDS(data_val[, c(response, predictors_bor)],\n        here::here(\"data/val_for_ph0-10.rds\"))\n```\n:::\n",
    "supporting": [
      "03-model_training_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}